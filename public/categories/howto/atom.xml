<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: HOWTO | OJ's rants]]></title>
  <link href="http://buffered.io/categories/howto/atom.xml" rel="self"/>
  <link href="http://buffered.io/"/>
  <updated>2012-04-04T19:41:48+10:00</updated>
  <id>http://buffered.io/</id>
  <author>
    <name><![CDATA[OJ Reeves]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Webmachine, ErlyDTL and Riak - Part 4]]></title>
    <link href="http://buffered.io/posts/webmachine-erlydtl-and-riak-part-4/"/>
    <updated>2012-02-15T20:50:00+10:00</updated>
    <id>http://buffered.io/posts/webmachine-erlydtl-and-riak-part-4</id>
    <content type="html"><![CDATA[<p>{% img left /uploads/2010/09/riak-logo.png 'Riak Logo' %}For those of you who are new to the series, you may want to check out <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a>, <a href="/posts/webmachine-erlydtl-and-riak-part-2/" title="Wembachine, ErlyDTL and Riak - Part 2">Part 2</a> and <a href="/posts/webmachine-erlydtl-and-riak-part-3/" title="Wembachine, ErlyDTL and Riak - Part 3">Part 3</a> before reading this post. It will help give you some context as well as introduce you to some of the jargon and technology that I'm using. If you've already read then, or don't want to, then please read on!</p>

<p>Upon finishing <a href="/posts/webmachine-erlydtl-and-riak-part-3/" title="Wembachine, ErlyDTL and Riak - Part 3">Part 3</a> of the series we were finally able to read data from <a href="http://www.basho.com/developers.html#Riak" title="Riak">Riak</a> and see it appear in our web page. This was the first stage in seeing a full end-to-end web application functioning. Of course there is still a great deal to do!</p>

<!--more-->


<h2>Agenda</h2>

<p>In this post we're going to hit a few points of pain:</p>

<ol>
<li>Another slight refactor! We need to manage Riak connections in a smarter way, so we'll do that first.</li>
<li>We'll be dealing with more configuration so we'll change the way our application deals with configuration so that it's all in the one spot and a little easier to manage.</li>
<li>Add the ability for users to sign in. To keep this simple and avoid the need for users to manage yet another login, we're going to use <a href="http://oauth.net/" title="OAuth">OAuth</a> and let people sign in with their <a href="http://twitter.com/" title="Twitter">Twitter</a> accounts.</li>
<li>Store a cookie in the user's browser which contains identifying information and an encrypted set of OAuth tokens.</li>
</ol>


<p>There's little Riak-specific work going on this post as we're focusing on front-end user management. Other than a bit of refactoring the Riak code remains the same as in Part 3. In Part 5 (coming soon) we'll be writing snippets to Riak and associating them to users who have logged into the application via Twitter.</p>

<p><strong>NOTE</strong>: I'll no longer be using <code>localhost</code> in URLs and will instead be using the loopback address, <code>127.0.0.1</code>. The main reason is because we'll be interacting with Twitter which requires a "proper" address to be used when setting up. A secondary reason is the use of cookies. If I accidentally leave <code>localhost</code> somewhere in the post (or in the images) please let me know.</p>

<p>Again, be warned, this post is a bit of a whopper! So get yourself a drink and get comfortable. Here we go...</p>

<h2>Another Slight Refactor</h2>

<p>Now that we're at the stage where Riak is going to get used more often we need to do a better job of handling and managing the connections to the cluster. Ideally we should pool a bunch of connections and reuse them across different requests. This reduces the overhead of creating and destroying connections all the time. Initially we're going to make use of Seth's <a href="https://github.com/seth/pooler" title="Pooler">Pooler</a> application (with a slight modification) to handle the pooling of Riak connections for us.</p>

<h3>Fixing HAProxy</h3>

<p>So now that we have a plan to pool connections, the first thing we need to fix is our load-balancer's configuration. At the moment we have configured <a href="http://haproxy.1wt.eu/" title="HAProxy">HAProxy</a> with the following settings:</p>

<p>{% codeblock dev.haproxy.conf lang:bash %}</p>

<h1>now set the default settings for each sub-section</h1>

<p>defaults
  .
  .
  # specify some timeouts (all in milliseconds)
  timeout connect 5000
  timeout client 50000
  timeout server 50000
  .
  .
{% endcodeblock %}</p>

<p>As you can see we've forced the timeout of connections which means that every connection that is made to the proxy will be killed off when it has been inactive for a long enough period of time. If you were paying attention to the output in the application console window you'd have seen something like this appear after making a request:</p>

<pre><code>=ERROR REPORT==== 13-Aug-2011::20:52:01 ===
** Generic server &lt;0.99.0&gt; terminating 
** Last message in was {tcp_closed,#Port&lt;0.2266&gt;}
** When Server state == {state,"127.0.0.1",8080,false,false,undefined,
                               undefined,
                               {[],[]},
                               1,[],infinity,100}
** Reason for termination == 
** disconnected

=CRASH REPORT==== 13-Aug-2011::20:52:01 ===
  crasher:
    initial call: riakc_pb_socket:init/1
    pid: &lt;0.99.0&gt;
    registered_name: []
    exception exit: disconnected
      in function  gen_server:terminate/6
    ancestors: [csd_core_server,csd_core_sup,&lt;0.52.0&gt;]
    messages: []
    links: [&lt;0.54.0&gt;]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 377
    stack_size: 24
    reductions: 911
  neighbours:
    neighbour: [{pid,&lt;0.54.0&gt;},
                  {registered_name,csd_core_server},
                  {initial_call,{csd_core_server,init,['Argument__1']}},
                  {current_function,{gen_server,loop,6}},
                  {ancestors,[csd_core_sup,&lt;0.52.0&gt;]},
                  {messages,[]},
                  {links,[&lt;0.53.0&gt;,&lt;0.99.0&gt;]},
                  {dictionary,[]},
                  {trap_exit,false},
                  {status,waiting},
                  {heap_size,987},
                  {stack_size,9},
                  {reductions,370}]

=SUPERVISOR REPORT==== 13-Aug-2011::20:52:01 ===
     Supervisor: {local,csd_core_sup}
     Context:    child_terminated
     Reason:     disconnected
     Offender:   [{pid,&lt;0.54.0&gt;},
                  {name,csd_core_server},
                  {mfargs,{csd_core_server,start_link,[]}},
                  {restart_type,permanent},
                  {shutdown,5000},
                  {child_type,worker}]


=PROGRESS REPORT==== 13-Aug-2011::20:52:01 ===
          supervisor: {local,csd_core_sup}
             started: [{pid,&lt;0.104.0&gt;},
                       {name,csd_core_server},
                       {mfargs,{csd_core_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]
</code></pre>

<p>This is paired up with the following output from the HAProxy console:</p>

<pre><code>00000010:riaks.srvcls[0009:000a]
00000010:riaks.clicls[0009:000a]
00000010:riaks.closed[0009:000a]
0000000e:webmachines.srvcls[0006:0007]
0000000e:webmachines.clicls[0006:0007]
0000000e:webmachines.closed[0006:0007]
</code></pre>

<p>These logs from the console clearly indicate that HAProxy is doing exactly what we've told it to do. It's killing off the connections after a period of time.</p>

<p>For a connection pool this is not a good idea. Therefore we need to modify this configuration so that it doesn't kill off connections. Thankfully this is a very simple thing to do! We delete the lines that force <code>client</code> and <code>server</code> timeouts (I'm commenting the lines out to make it obvious which ones you need to remove):</p>

<p>{% codeblock dev.haproxy.conf lang:bash %}</p>

<h1>now set the default settings for each sub-section</h1>

<p>defaults
  .
  .
  # specify some timeouts (all in milliseconds)
  timeout connect 5000
  #timeout client 50000
  #timeout server 50000
  .
  .
{% endcodeblock %}</p>

<p>After making this change to the configuration, HAProxy will no longer kill off the connections. Therefore it's up to us to manage them.</p>

<h3>Connection Pooling</h3>

<p>Given that it is <em>not</em> one of the goals of this series to demonstrate how to create a connection pooling application in Erlang, we're going to use an application that's already out there to do it for us. This application is called <a href="https://github.com/seth/pooler" title="Pooler">Pooler</a>. Out of the box this application does Erlang process pooling, and given that our Riak connections are each Erlang processes, this suits us perfectly.</p>

<p>One thing that I didn't like about the interface to Pooler was that it relied on the caller managing the lifetime of the connection. As a result, I made a small change to the interface in my own <a href="https://github.com/OJ/pooler" title="OJ's Pooler fork">fork</a> which I think helps keep things a little cleaner. This application will be making use of this fork.</p>

<p>First up, we need to add another dependency in our <code>rebar.config</code> file which will pull this application in from Github at a dependency.</p>

<p>{% codeblock apps/csd_core/rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{deps,
  [</p>

<pre><code>{mochiweb, ".*", {git, "git://github.com/mochi/mochiweb", "HEAD"}},
{riakc, ".*", {git, "git://github.com/basho/riak-erlang-client", "HEAD"}},
{pooler, ".*", {git, "git://github.com/OJ/pooler", "HEAD"}}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>Build the application so that the dependency is pulled and built:</p>

<p>{% codeblock lang:bash %}
oj@hitchens ~/code/csd $ make</p>

<p>   ... snip ...</p>

<p>Pulling pooler from {git,"git://github.com/OJ/pooler","HEAD"}
Cloning into pooler...
==> pooler (get-deps)</p>

<p>   ... snip ...</p>

<p>==> pooler (compile)
Compiled src/pooler_app.erl
Compiled src/pooler_pooled_worker_sup.erl
Compiled src/pooler_pool_sup.erl
Compiled src/pooler_sup.erl
Compiled src/pooler.erl</p>

<p>   ... snip ...
{% endcodeblock %}</p>

<p>Next we need to take the scalpel to <code>csd_core</code>. When we first created this application, it was intended to manage all of the interaction with Riak and to manage the intricacies of dealing with snippets and other objects without exposing Riak's inner workings to the <code>csd_web</code> application. To do this we put a <a href="http://www.erlang.org/doc/man/gen_server.html" title="gen_server">gen_server</a> in place, called <code>csd_core_server</code>, which handled the incoming requests. It internally established connections to Riak and used them without destroying them.</p>

<p>For now, we'll be keeping this <code>gen_server</code> in place but we're going to make some modifications to it:</p>

<ol>
<li>We'll start and stop <code>pooler</code> when our <code>csd_core</code> application starts and stops.</li>
<li>We'll change the way configuration is managed and add the configuration for <code>pooler</code>.</li>
<li>We'll be removing the code that establishes the connections.</li>
<li>We'll pass the calls through to Riak using the new <code>pooler</code> application.</li>
</ol>


<p>Let's get to it.</p>

<h4>Starting and Stopping Pooler</h4>

<p>Given that we're using <code>pooler</code> the first thing we need to do is make sure that it loads and runs when <code>csd_core</code> fires up. To do this, we need to modify <code>csd_core.erl</code> so that it looks like this:</p>

<p>{% codeblock apps/csd_core/src/csd_core.erl lang:erlang %}
%% @author OJ Reeves <a href="&#x6d;&#97;&#x69;&#108;&#116;&#111;&#x3a;&#111;&#x6a;&#x40;&#98;&#117;&#x66;&#x66;&#101;&#114;&#x65;&#x64;&#46;&#105;&#x6f;">&#x6f;&#106;&#x40;&#98;&#x75;&#102;&#x66;&#101;&#114;&#x65;&#100;&#46;&#105;&#111;</a>
%% @copyright 2011 OJ Reeves</p>

<p>%% @doc csd_core startup code</p>

<p>-module(csd_core).
-author('OJ Reeves <a href="&#x6d;&#97;&#x69;&#x6c;&#116;&#x6f;&#58;&#111;&#106;&#x40;&#98;&#117;&#102;&#x66;&#101;&#114;&#101;&#x64;&#46;&#x69;&#111;">&#x6f;&#106;&#x40;&#x62;&#117;&#102;&#x66;&#x65;&#x72;&#x65;&#100;&#x2e;&#x69;&#x6f;</a>').
-export([start/0, start_link/0, stop/0]).</p>

<p>ensure_started(App) -></p>

<pre><code>case application:start(App) of
    ok -&gt;
        ok;
    {error, {already_started, App}} -&gt;
        ok
end.
</code></pre>

<p>%% @spec start_link() -> {ok,Pid::pid()}
%% @doc Starts the app for inclusion in a supervisor tree
start_link() -></p>

<pre><code>start_common(),
csd_core_sup:start_link().
</code></pre>

<p>%% @spec start() -> ok
%% @doc Start the csd_core server.
start() -></p>

<pre><code>start_common(),
application:start(csd_core).
</code></pre>

<p>%% @spec stop() -> ok
%% @doc Stop the csd_core server.
stop() -></p>

<pre><code>Res = application:stop(csd_core),
application:stop(pooler),
application:stop(crypto),
Res.
</code></pre>

<p>%% @private
start_common() -></p>

<pre><code>ensure_started(crypto),
ensure_started(pooler).
</code></pre>

<p>{% endcodeblock %}</p>

<p>This code will start and stop the <code>pooler</code> application along with our application. Exactly what we need!</p>

<h4>Fixing Configuration</h4>

<p>Our rudimentary configuration module, <code>csd_riak_config.erl</code>, is now obsolete. We're going to remove it and replace it with something a little more complicated which will not only make it easier to handle configuration using Erlang's built-in <a href="http://www.erlang.org/doc/man/config.html" title="Erlang configuration">configuration</a> handling, but we'll add some code which will make it easier to access configuration both in development <em>and</em> once the application has been deployed.</p>

<p>Let's start by creating a new file:</p>

<p>{% codeblock apps/csd_core/priv/app.config lang:erlang %}
[
  {pooler, [</p>

<pre><code>  {pools, [
      [
        {name, "haproxy"},
        {max_count, 30},
        {init_count, 5},
        {start_mfa, {riakc_pb_socket, start_link, ["127.0.0.1", 8080]}}
      ]
    ]}
]}
</code></pre>

<p>].
{% endcodeblock %}</p>

<p><code>pooler</code> is smart enough to pool connections across multiple nodes. This is quite a nifty feature, but not one that we're making use of because we have HAProxy in place. Therefore, the configuration above is telling Pooler to use just one single node/pool (ie. the proxy), to create 5 connections and to allow up to 30 to be created if required.</p>

<p>The last parameter in the configuration, <code>start_mfa</code>, tells <code>pooler</code> which module, function and arguments to invoke to create the Erlang process from. In our case we want it to create a pool of Riak client connections, hence why we've specified the <code>start_link</code> function in the <code>riakc_pb_socket</code> module.</p>

<p>Next we modify our <code>Makefile</code> so that when we invoke <code>make webstart</code> the configuration is properly included:</p>

<p>{% codeblock Makefile lang:bash %}
.PHONY: deps</p>

<p>REBAR=<code>which rebar || ./rebar</code></p>

<p>all: deps compile</p>

<p>compile:</p>

<pre><code>@$(REBAR) compile
</code></pre>

<p>app:</p>

<pre><code>@$(REBAR) compile skip_deps=true
</code></pre>

<p>deps:</p>

<pre><code>@$(REBAR) get-deps
</code></pre>

<p>clean:</p>

<pre><code>@$(REBAR) clean
</code></pre>

<p>distclean: clean</p>

<pre><code>@$(REBAR) delete-deps
</code></pre>

<p>test: app</p>

<pre><code>@$(REBAR) eunit skip_deps=true
</code></pre>

<p>webstart: app</p>

<pre><code>exec erl -pa $(PWD)/apps/*/ebin -pa $(PWD)/deps/*/ebin -boot start_sasl -config $(PWD)/apps/csd_core/priv/app.config -s reloader -s csd_core -s csd_web
</code></pre>

<p>proxystart:</p>

<pre><code>@haproxy -f dev.haproxy.conf
</code></pre>

<p>{% endcodeblock %}</p>

<p>At this point we are able to build and run the application just as we were before. The first thing you'll notice is that the HAProxy console immediately registers 5 new connections:</p>

<p>{% codeblock lang:bash %}
0000004:dbcluster.accept(0005)=0006 from [127.0.0.1:34536]
00000005:dbcluster.accept(0005)=0008 from [127.0.0.1:58770]
00000006:dbcluster.accept(0005)=000a from [127.0.0.1:44734]
00000007:dbcluster.accept(0005)=000c from [127.0.0.1:33874]
00000008:dbcluster.accept(0005)=000e from [127.0.0.1:35815]
{% endcodeblock %}</p>

<p>This is evidence that <code>pooler</code> is doing its job and starting with 5 connections. Now that we have this in place, let's get rid of the old configuration:</p>

<p>{% codeblock lang:bash %}
oj@hitchens ~/code/csd $ rm apps/csd_core/src/csd_riak_config.erl
{% endcodeblock %}</p>

<p>That was easy! We now need to remove any references to this module, thankfully the only module that used was <code>csd_core_server.erl</code>, and that's the one we're going to fix up now. After removing references to the configuration, removing connection creation and replacing it with calls to <code>pooler</code>, <code>csd_core_server</code> now looks like this:</p>

<p>{% codeblock apps/csd_core/src/csd_core_server.erl lang:erlang %}
-module(csd_core_server).
-behaviour(gen_server).
-define(SERVER, ?MODULE).</p>

<p>%% ------------------------------------------------------------------
%% API Function Exports
%% ------------------------------------------------------------------</p>

<p>-export([start_link/0, get_snippet/1, save_snippet/1]).</p>

<p>%% ------------------------------------------------------------------
%% gen_server Function Exports
%% ------------------------------------------------------------------</p>

<p>-export([init/1, handle_call/3, handle_cast/2, handle_info/2, terminate/2, code_change/3]).</p>

<p>%% ------------------------------------------------------------------
%% API Function Definitions
%% ------------------------------------------------------------------</p>

<p>start_link() ->
  gen_server:start_link({local, ?SERVER}, ?MODULE, [], []).</p>

<p>save_snippet(Snippet) ->
  gen_server:call(?SERVER, {save_snippet, Snippet}, infinity).</p>

<p>get_snippet(SnippetKey) ->
  gen_server:call(?SERVER, {get_snippet, SnippetKey}, infinity).</p>

<p>%% ------------------------------------------------------------------
%% gen_server Function Definitions
%% ------------------------------------------------------------------</p>

<p>init([]) ->
  {ok, undefined}.</p>

<p>handle_call({save_snippet, Snippet}, _From, State) ->
  SavedSnippet = pooler:use_member(fun(RiakPid) -> csd_snippet:save(RiakPid, Snippet) end),
  {reply, SavedSnippet, State};</p>

<p>handle_call({get_snippet, SnippetKey}, _From, State) ->
  Snippet = pooler:use_member(fun(RiakPid) -> csd_snippet:fetch(RiakPid, SnippetKey) end),
  {reply, Snippet, State};</p>

<p>handle_call(<em>Request, </em>From, State) ->
  {noreply, ok, State}.</p>

<p>handle_cast(_Msg, State) ->
  {noreply, State}.</p>

<p>handle_info(_Info, State) ->
  {noreply, State}.</p>

<p>terminate(<em>Reason, </em>State) ->
  ok.</p>

<p>code_change(<em>OldVsn, State, </em>Extra) ->
  {ok, State}.
{% endcodeblock %}</p>

<p>Here you can see we're making use of the <a href="https://github.com/OJ/pooler/blob/master/src/pooler.erl#L125" title="use_member">pooler:use_member</a> function to easily wrap up the management of the connection's usage lifetime. All traces of the old configuration are gone. We can now rebuild the application using <code>make</code>, fire it up using <code>make webstart</code> and hit the <a href="http://127.0.0.1/snippet/B41kUQ==">same page</a> as before resulting in the same content appearing on screen.</p>

<p>We have now successfully removed the old configuration and connection handling code, and we've replaced it with <code>pooler</code> to handle a pool of connections to the Riak proxy. The last part of our refactor is around configuration for the front-end web application.</p>

<h2>Rewiring Configuration</h2>

<p>Our configuration is going to get more complicated, so to make sure that we're able to better handle and manage it we're going to set up a similar structure to what we had set up in the <code>csd_core</code> application (in the previous section). The first thing we're going to change is the way that the <strong>Webmachine</strong> routes are loaded. Right now, they're stored in <code>apps/tr_web/priv/dispatch.conf</code>. This configuration belongs alongside others, so we'll move that to an <code>app.config</code> file and re-jig the code to load it from there.</p>

<p>First up, rename the file:</p>

<pre><code>oj@air ~/code/csd/apps/csd_web/priv $ mv dispatch.conf app.config
</code></pre>

<p>Now let's edit it so that it takes the appropriate format:</p>

<p>{% codeblock apps/csd_web/priv/app.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
[
  {sasl,</p>

<pre><code>[
  {sasl_error_logger, {file, "log/sasl-error.log"}},
  {errlog_type, error},
  {error_logger_mf_dir, "log/sasl"},      % Log directory
  {error_logger_mf_maxbytes, 10485760},   % 10 MB max file size
  {error_logger_mf_maxfiles, 5}           % 5 files max
]
</code></pre>

<p>  },
  {csd_web,</p>

<pre><code>[
  {web,
    [
      {ip, "0.0.0.0"},
      {port, 8000},
      {log_dir, "priv/log"},
      {dispatch,
        [
          {[], csd_web_resource, []},
          {["snippet", key], csd_web_snippet_resource, []}
        ]
      }
    ]
  }
]
</code></pre>

<p>  }
].
{% endcodeblock %}</p>

<p>A few things to note here:</p>

<ol>
<li>I've included the <code>sasl</code> configuration for later tweaking.</li>
<li>the <code>csd_web</code> section is named that way so that it is matches the application name. This makes the auto-wiring work.</li>
<li>The Webmachine configuration for application is now in a subsection called <code>web</code>. Inside this section is the original <code>dispatch</code> that we had in our old <code>dispatch.conf</code>. This configuration sections takes the <em>exact</em> form that Webmachine expects when we start its process in our supervisor.</li>
</ol>


<p>At this point we need to go and fiddle with the way Webmachine loads its configuration so that it picks up these details. We'll start by defining a helper which will make it easy to get access to configuration for the <code>csd_web</code> application.</p>

<p>{% codeblock apps/csd_web/src/conf.erl lang:erlang %}
-module(conf).</p>

<p>-export([get_section/1, get_section/2]).
-export([get_val/2, get_val/3]).</p>

<p>get_section(Name) ->
  get_section(Name, undefined).</p>

<p>get_section(Name, Default) ->
  case application:get_env(csd_web, Name) of</p>

<pre><code>{ok, V} -&gt;
  V;
_ -&gt;
  Default
</code></pre>

<p>  end.</p>

<p>get_val(SectionName, Name) ->
  get_val(SectionName, Name, undefined).</p>

<p>get_val(SectionName, Name, Default) ->
  case application:get_env(csd_web, SectionName) of</p>

<pre><code>{ok, Section} -&gt;
  proplists:get_value(Name, Section, Default);
_ -&gt;
  Default
</code></pre>

<p>  end.
{% endcodeblock %}</p>

<p>Configuration helpers are now in place, let's fix the Webmachine loader in <code>csd_web_sup.erl</code>.</p>

<p>{% codeblock apps/csd_web/src/csd_web_sup.erl (partial) lang:erlang %}
% ... snip ... %
%% @spec init([]) -> SupervisorTree
%% @doc supervisor callback.
init([]) ->
  WebConfig = conf:get_section(web),
  Web = {webmachine_mochiweb,</p>

<pre><code>{webmachine_mochiweb, start, [WebConfig]},
permanent, 5000, worker, dynamic},
</code></pre>

<p>  Processes = [Web],
  {ok, { {one_for_one, 10, 10}, Processes} }.
% ... snip ... %
{% endcodeblock %}</p>

<p>This little snippet delegates the responsibility of all Webmachine-related stuff to the <code>app.config</code> file. Let's include this in our <code>Makefile</code> when we start our application.</p>

<p>{% codeblock Makefile (partial) lang:bash %}
webstart: app</p>

<pre><code>exec erl -pa $(PWD)/apps/*/ebin -pa $(PWD)/deps/*/ebin -boot start_sasl -config $(PWD)/apps/csd_web/priv/app.config -config $(PWD)/apps/csd_core/priv/app.config -s reloader -s csd_core -s csd_web
</code></pre>

<p>{% endcodeblock %}</p>

<p>All we've done here is add another <code>-config</code> parameter and pointed it at the new <code>app.config</code> file in the <code>csd_web/src</code> folder. Fire up the application and it <em>should</em> behave exactly as it did before.</p>

<p>Now that we have our configuration tweaked we have finalised the last of the refactoring tasks (at least for now). It's now time to start designing our user login functionality.</p>

<h2>Handling User Logins</h2>

<p>Handling logins isn't necessarily as simple as it looks. Remember, <a href="http://www.basho.com/developers.html#Webmachine" title="Webmachine">Webmachine</a> is not a Web application framework, it's a feature-rich tool which helps us build well-behaving RESTful HTTP applications. The idea of a "session" is a (leaky) abstraction that web developers have added to web applications to aid in preventing users from having to manually sign in each time they want to access a resource. This abstraction tends to be handled through cookies.</p>

<p>We'll be doing the same, but given that we don't have anything in place at all we're going to have to come up with our own method for handling authentication of the user via cookies.</p>

<p>Bearing in mind that we'll be making use of Twitter, via OAuth, to deal with the process of authentication, the login process will consist of the following steps:</p>

<ol>
<li>The user clicks a "login via Twitter" button.</li>
<li>The server handles the request and negotiates a <a href="http://oauth.net/core/1.0/#auth_step1" title="Request tokens">request token</a> with Twitter using OAuth.</li>
<li>The application redirects the user to Twitter on a special URL which contains OAuth request information.</li>
<li>The user is asked to sign in to Twitter, if they haven't already during the course of their browser session.</li>
<li>Twitter then confirms that the user does intend to sign-in to Code Smackdown using their Twitter credentials, and redirects the user back to the application.</li>
<li>If the user approves the process, the application is handed a verification token which is then used to generate an OAuth <a href="http://oauth.net/core/1.0/#auth_step3" title="Access tokens">access token</a> with Twitter. This access token is what is used to allow the user to easily sign in to the application from this point onward.</li>
</ol>


<p>Prepare yourself, you're about to learn how to do OAuth in Erlang! But before we can do that, we need to register our application with Twitter.</p>

<h3>Creating a new Twitter Application</h3>

<p>Start by browsing to the <a href="https://dev.twitter.com/apps/new" title="New Twitter Application">Twitter application registration page</a> and signing in with your Twitter account credentials. You'll be taken to a page where you can enter the details of the application. Set the <strong>Callback URL</strong> to <code>http://127.0.0.1:4000/oauth/callback</code> for now. This points the Twitter redirect traffic back to localhost which will make things easy during development. When it comes time to deploy the application to production you can change this to the proper callback address.</p>

<p><img src="/uploads/2012/02/twitter-app-create.png" title="Twitter app creation" alt="Creating an application in Twitter" /></p>

<p>Once you've filled out the details you'll being presented with a standard set of OAuth-related bits which we'll be using down the track. I'll of course be using my own registered application name (Code Smackdown) along with the keys. Given these keys are specific to my application and should be kept secret I will not be making them part of the source (sorry).</p>

<p>Once you're registered, we're ready to take the OAuth configuration information from Twitter and plug it into our own configuration. Re-open <code>csd_web/priv/app.config</code> and create a new section called <code>twitter</code> under the <code>csd_web</code> section and add the following</p>

<p>{% codeblock apps/csd_web/priv/app.config (partial) lang:erlang %}
% ... snip ... %
  {csd_web,</p>

<pre><code>[
  % ... snip ... %
  {twitter,
    [
      {consumer_key, "&lt; your application's key goes here &gt;"},
      {consumer_secret, "&lt; your application's secret goes here &gt;"},
      {request_token_url, "https://twitter.com/oauth/request_token"},
      {access_token_url, "https://twitter.com/oauth/access_token"},
      {authenticate_url, "https://twitter.com/oauth/authenticate"},
      {current_user_info_url, "https://twitter.com/account/verify_credentials.json"}
    ]
  }
]
</code></pre>

<p>  }
% ... snip ... %
{% endcodeblock %}</p>

<p>The first two values come straight from Twitter and would have been given to you upon registering your application. The rest are URLs that we'll be using later on when doing the OAuth handshake.</p>

<p>Now that we've got our configuration locked in we can get started on managing the requests. For this we need to understand how OAuth actually works.</p>

<p>A deep-dive into the ins and outs of OAuth is beyond the scope of this article. I recommend having a read of <a href="http://www.slideshare.net/leahculver/oauth-open-api-authentication" title="OAuth overview">this presentation on OAuth</a> which gives a good overview. The rest of this article will fill the gaps as to how it all works.</p>

<h3>Implementing OAuth</h3>

<p>Using OAuth requires us to invoke HTTP requests to Twitter. We could go through the pain of doing this manually, but instead we're going to use another Open Source utility which has the ability to handle this for us.</p>

<p><a href="https://github.com/tim/erlang-oauth" title="erlang-oauth">erlang-oauth</a> is an Erlang application which makes it easy to deal with OAuth requests and is ideal for what we need to do. Given that it will be a dependency on our application we need it to work nicely with rebar. Out of the box this isn't the case, so I have made a <a href="https://github.com/OJ/erlang-oauth/tree/rebarise" title="erlang-oauth rebar fork">fork</a> with a topic branch that has rebar-friendliness in it. We'll use this fork and branch in our application.</p>

<p>{% codeblock apps/csd_web/rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{deps,
  [</p>

<pre><code>{oauth, ".*", {git, "git://github.com/OJ/erlang-oauth", {branch, "rebarise"}}},
{webmachine, ".*", {git, "git://github.com/basho/webmachine", "HEAD"}},
{erlydtl, ".*", {git, "git://github.com/OJ/erlydtl.git", "HEAD"}}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>The <code>erlang-oauth</code> application requires <code>ssl</code> and <code>public_key</code> applications to be running for it to function properly, so we need to kick those applications off during start-up. We can do that by editing <code>csd_web.erl</code> like so:</p>

<p>{% codeblock apps/csd_web/src/csd_web.erl lang:erlang %}</p>

<p>%% @author OJ Reeves <a href="&#x6d;&#97;&#x69;&#x6c;&#116;&#111;&#x3a;&#111;&#106;&#x40;&#98;&#117;&#x66;&#x66;&#101;&#x72;&#x65;&#x64;&#x2e;&#x69;&#x6f;">&#x6f;&#x6a;&#64;&#98;&#117;&#102;&#102;&#101;&#x72;&#101;&#x64;&#x2e;&#x69;&#x6f;</a>
%% @copyright 2012 OJ Reeves.</p>

<p>%% @doc csd_web startup code</p>

<p>-module(csd_web).
-author('OJ Reeves <a href="&#x6d;&#97;&#105;&#x6c;&#116;&#x6f;&#x3a;&#x6f;&#x6a;&#64;&#x62;&#117;&#102;&#x66;&#x65;&#x72;&#101;&#100;&#46;&#105;&#x6f;">&#x6f;&#106;&#x40;&#x62;&#x75;&#x66;&#102;&#101;&#x72;&#x65;&#x64;&#x2e;&#105;&#111;</a>').
-export([start/0, start_link/0, stop/0]).</p>

<p>ensure_started(App) ->
  case application:start(App) of</p>

<pre><code>ok -&gt;
  ok;
{error, {already_started, App}} -&gt;
  ok
</code></pre>

<p>  end.</p>

<p>%% @spec start_link() -> {ok,Pid::pid()}
%% @doc Starts the app for inclusion in a supervisor tree
start_link() ->
  start_common(),
  csd_web_sup:start_link().</p>

<p>%% @spec start() -> ok
%% @doc Start the csd_web server.
start() ->
  start_common(),
  application:start(csd_web).</p>

<p>%% @spec stop() -> ok
%% @doc Stop the csd_web server.
stop() ->
  Res = application:stop(csd_web),
  application:stop(webmachine),
  application:stop(mochiweb),
  application:stop(public_key), % stop new dependency
  application:stop(ssl),        % stop new dependency
  application:stop(crypto),
  application:stop(inets),
  Res.</p>

<p>start_common() ->
  ensure_started(inets),
  ensure_started(crypto),
  ensure_started(public_key), % start new dependency
  ensure_started(ssl),        % start new dependency
  ensure_started(mochiweb),
  application:set_env(webmachine, webmachine_logger_module, webmachine_logger),
  ensure_started(webmachine),
  ok.
{% endcodeblock %}</p>

<p>Interacting with Twitter now becomes quite simple. To handle talking to Twitter we'll create a new module, called <code>twitter.erl</code>, that does the dirty work. Let's take a look at the code then we'll walk through it.</p>

<p>{% codeblock apps/csd_web/src/csd_web.erl lang:erlang %}</p>

<p>%% @author OJ Reeves <a href="&#x6d;&#x61;&#x69;&#108;&#116;&#111;&#58;&#111;&#106;&#x40;&#x62;&#x75;&#x66;&#102;&#x65;&#x72;&#x65;&#100;&#46;&#105;&#x6f;">&#x6f;&#106;&#64;&#98;&#x75;&#x66;&#x66;&#101;&#x72;&#101;&#x64;&#x2e;&#x69;&#111;</a>
%% @copyright 2012 OJ Reeves</p>

<p>-module(twitter).</p>

<p>-author('OJ Reeves <a href="&#x6d;&#x61;&#105;&#108;&#x74;&#x6f;&#x3a;&#111;&#106;&#x40;&#98;&#x75;&#102;&#102;&#x65;&#x72;&#101;&#x64;&#x2e;&#105;&#x6f;">&#111;&#x6a;&#64;&#98;&#x75;&#102;&#x66;&#101;&#114;&#x65;&#x64;&#x2e;&#x69;&#x6f;</a>').</p>

<p>-export([request_access/0, verify_access/3, get_current_user_info/2]).</p>

<p>request_access() ->
  TwitterConf = conf:get_section(twitter),
  RequestTokenUrl = proplists:get_value(request_token_url, TwitterConf),
  {ok, RequestResponse} = oauth:get(RequestTokenUrl, [], consumer(TwitterConf)),
  RequestParams = oauth:params_decode(RequestResponse),
  RequestToken = oauth:token(RequestParams),
  AuthenticateUrl = proplists:get_value(authenticate_url, TwitterConf),
  {ok, oauth:uri(AuthenticateUrl, [{"oauth_token", RequestToken}])}.</p>

<p>verify_access(RequestToken, RequestTokenSecret, Verifier) ->
  TwitterConf = conf:get_section(twitter),
  AccessTokenUrl = proplists:get_value(access_token_url, TwitterConf),
  {ok, AccessResponse} = oauth:get(AccessTokenUrl, [{"oauth_verifier", Verifier}], consumer(TwitterConf), RequestToken, RequestTokenSecret),
  AccessParams = oauth:params_decode(AccessResponse),
  AccessToken = oauth:token(AccessParams),
  AccessTokenSecret = oauth:token_secret(AccessParams),
  {ok, AccessToken, AccessTokenSecret}.</p>

<p>get_current_user_info(AccessToken, AccessTokenSecret) ->
  call_json_service(current_user_info_url, AccessToken, AccessTokenSecret).</p>

<p>% Extract a oauth-formatted consumer tuple from the given Twitter configuration.
consumer(TwitterConf) ->
  ConsumerKey = proplists:get_value(consumer_key, TwitterConf),
  ConsumerSecret = proplists:get_value(consumer_secret, TwitterConf),
  {ConsumerKey, ConsumerSecret, hmac_sha1}.</p>

<p>% Invoke a call to a JSON service on Twitter.
call_json_service(UrlKey, AccessToken, AccessTokenSecret) ->
  TwitterConf = conf:get_section(twitter),
  Url = proplists:get_value(UrlKey, TwitterConf),
  {ok, Response} = oauth:get(Url, [], consumer(TwitterConf), AccessToken, AccessTokenSecret),
  {{ "{{" }}<em>Version, 200, "OK"}, </em>Headers, Json} = Response,
  {ok, Json}.
{% endcodeblock %}</p>

<p>This might seem like a lot but there isn't much to it. Here's the run-down:</p>

<ul>
<li><p><code>request_access</code>: This function is what handles the first step in the OAuth negotiation process. It starts by loading the <code>twitter</code> configuration from our <code>app.config</code> file. The <code>twitter</code> section contains all the URLs we need to talk to Twitter</p>

<p>First we need to get hole of a <em>request token</em>, which is an identifier for an authorisation request that Twitter generates when we first start talking OAuth. We get th <code>request_token_url</code> from the configuration and we connect to Twitter, using <code>oauth:get</code> to kick the process off. Note the use of the <code>consumer</code> function, which simply takes our local <code>twitter</code> configuration and populates an <code>erlang-oauth</code>-friendly tuple with the details required to make OAuth requests on behalf of our application. This tuple contains our <em>consumer key</em>, the <em>consumer secret</em> and the signature method to use. We will always be using <code>hmac_sha1</code> as that's what Twitter currently requires.</p>

<p>Twitter reponds with a payload which includes the generated request token. We take that request token out of the payload and generate an Authentication URL. This URL contains information about the request that we started in the previous steps, along with the <code>authenticate_url</code> value loaded from configuration. If you remember back to our configuration you'll see that this <code>authenticate_url</code> is one that Twitter told us to use when we first registered our application and it resolves to <code>https://twitter.com/oauth/authenticate</code>.</p>

<p>This URL is returned to the caller and the calling code should redirect the user to this URL so that they can authenticate themselves with Twitter.</p></li>
<li><p><code>verify_access</code>: This function is what is called after the use has authenticated themselves with Twitter. The function expects both the <em>request token</em> and <em>request token secret</em> so that the result of the request can be validated with Twitter. Twitter also generates a "verifier" value as part of it's authentication process, and this value is what is passed in via the <code>Verifier</code> parameter.</p>

<p>After getting hold of the Twitter configuration an <em>access token</em> URL is generated. This URL contains all the information required to turn the <em>request token</em> into an <em>access token</em>. Once generated, this URL is then accessed via <code>erlang-oauth</code> and the payload that comes back from Twitter contains both the <em>access token</em> and the <em>access token secret</em>. Both of these are required from this point on to make requests to Twitter on behalf of the user.</p></li>
<li><p><code>get_current_user_info</code>: This is a small helper function which calls to Twitter via <code>erlang-oauth</code> and extracts the user details for the user. The payload contains the usual Twitter profile stuff such as Twitter ID, username, bio, tweet count, etc.</p></li>
</ul>


<p>Before we take a look at the Webmachine resource that will invoke this functionality, let's take a look at what we'll need to do with the tokens once we've got them.</p>

<p>For now, we are only going to store them, encrypted, in the user's cookie which we'll send down to the browser. This isn't "best practice" when it comes to storage of this kind of information, but for the sake of this blog post it will suffice. Later in the series we'll be doing more with this information and most likely removing some of the information from the cookie.</p>

<p>With this in mind, we need something that is able to write to and read from the user's cookies during a request. This module needs to be able to verify that a user's cookie is valid and that it hasn't expired. When writing and reading the module must also handle the encryption of the sensitive information.</p>

<p>Let's create this new module, called <code>cookie.erl</code>, inside <code>csd_web</code>. I'll break it up into it's functions so you can see what it's doing.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}</p>

<p>%% @author OJ Reeves <a href="&#x6d;&#97;&#x69;&#x6c;&#116;&#111;&#x3a;&#111;&#x6a;&#64;&#x62;&#117;&#102;&#102;&#x65;&#114;&#x65;&#x64;&#x2e;&#105;&#x6f;">&#111;&#106;&#64;&#98;&#117;&#102;&#102;&#101;&#x72;&#x65;&#x64;&#46;&#105;&#111;</a>
%% @copyright 2012 OJ Reeves</p>

<p>-module(cookie).</p>

<p>-author('OJ Reeves <a href="&#109;&#x61;&#105;&#108;&#116;&#111;&#58;&#111;&#106;&#64;&#x62;&#117;&#102;&#102;&#x65;&#114;&#x65;&#100;&#46;&#x69;&#111;">&#x6f;&#x6a;&#64;&#x62;&#x75;&#102;&#102;&#x65;&#114;&#101;&#x64;&#x2e;&#105;&#111;</a>').</p>

<p>-export([load_auth/1, store_auth/5]).</p>

<p>-define(AUTH_COOKIE, "<strong>CodeSmackdown</strong>Auth").
-define(AUTH_SALT, "27ed2d041cdb4b8b2702").
-define(AUTH_SECRET, "2d0431cd9bda5ba4b98271edcb2e7102").
-define(AUTH_EXPIRY_DAYS, 7).
-define(ENC_IV, &lt;&lt;207,94,217,158,198,63,132,205,35,187,246,2,56,122,250,33>>).
-define(ENC_KEY,
  &lt;&lt;110,56,121,28,235,159,77,154,160,5,130,210,204,32,26,224,255,86,101,71,61,3,
  66,69,30,39,42,0,116,93,204,99>>).
{% endcodeblock %}</p>

<p>Ignoring the usual headers/setup for the module, we can see a stack of defines. They are:</p>

<ul>
<li><code>AUTH_COOKIE</code>: This is the name of the cookie that will live in the browser. If you use a cookie editor you'll see this name appear as the name of the cookie once it's written.</li>
<li><code>AUTH_SALT</code>: This is a bunch of characters that will be used as a <a href="http://en.wikipedia.org/wiki/Salt_(cryptography)" title="Salt (crypto)">salt</a> for when we're generating the <a href="http://en.wikipedia.org/wiki/HMAC" title="HMAC">SHA MAC</a> from the user's cookie information.</li>
<li><code>AUTH_SECRET</code>: This is the key we'll be using when creating a <a href="http://en.wikipedia.org/wiki/HMAC" title="HMAC">SHA MAC</a> from the data we'll be pushing into the cookie. This is to make sure that the cookie hasn't been tampered with.</li>
<li><code>AUTH_EXPIRY_DAYS</code>: This is the number of days that the cookie is valid for.</li>
<li><code>ENC_IV</code>: This is the initialisation vector used when encrypting/decrypting the data in the cookie.</li>
<li><code>ENC_KEY</code>: This is the key that's used for encrypting/decrypting data that's in the cookie.</li>
</ul>


<p>Pretty simple stuff. Now let's take a look at a function that does something interesting.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}
load_auth(ReqData) ->
  case wrq:get_cookie_value(?AUTH_COOKIE, ReqData) of</p>

<pre><code>undefined -&gt;
  {error, no_cookie};
V -&gt;
  Val = mochiweb_util:unquote(V),
  decode(Val)
</code></pre>

<p>  end.
{% endcodeblock %}</p>

<p><code>load_auth</code> is a function which attempts to load authentication information from the cookies stored in the <code>ReqData</code> parameter. <code>ReqData</code> is the <a href="http://wiki.basho.com/Webmachine-Request.html" title="Request data">request data</a> that comes from Webmachine. As you can see, the function attempts to read the cookie value from the request data using Webmachine's <a href="http://wiki.basho.com/Webmachine-Request.html" title="Request data">wrq</a> module. If it fails <code>undefined</code> is returned and we know that no cookie has been set. If a value is read, we munge the data into something usable and then attempt to decode it using the <code>decode</code> function explained further down.</p>

<p>This function returns either <code>{ok, &lt;Cookie Information&gt;}</code> or <code>{error, &lt;Reason&gt;}</code>.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}
store_auth(ReqData, Id, Name, Token, TokenSecret) ->
  Value = mochiweb_util:quote_plus(encode(Id, Name, Token, TokenSecret)),
  Options = [</p>

<pre><code>%{domain, "codesmackdown.com"},
{max_age, 3600 * 24 * ?AUTH_EXPIRY_DAYS},
{path, "/"},
{http_only, true}
</code></pre>

<p>  ],
  CookieHeader = mochiweb_cookies:cookie(?AUTH_COOKIE, Value, Options),
  wrq:merge_resp_headers([CookieHeader], ReqData).
{% endcodeblock %}</p>

<p><code>store_auth</code> is the opposite to <code>load_auth</code> as it writes the user's information and token data to a cookie. The parameters to this function are:</p>

<ul>
<li><code>ReqData</code>: Webmachine's request data.</li>
<li><code>Id</code>: The user's Twitter ID. We'll be using this as a key later on to retrieve information from Riak.</li>
<li><code>Name</code>: The user's Twitter user name. We'll use this purely for display.</li>
<li><code>Token</code> and <code>TokenSecret</code>: Token information for making OAuth requests on behalf of this user.</li>
</ul>


<p>The first thing we do is call <code>encode</code> and pass in the last four arguments. This gives us an encrypted blob which we can store in a cookie. We then put down some basic information inside <code>Options</code>, including the expiry date. We then use <code>mochiweb_cookies</code> to generate a cookie with the name (<code>AUTH_COOKIE</code>), value and options.</p>

<p>Lastly we take the generated cookie header and merge that with the headers that already part of <code>ReqData</code> and produce a new request data object which is returned to the caller.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}
encode(Id, Name, Token, TokenSecret) ->
  SecretInfo = encrypt({Token, TokenSecret}),
  CookieValue = {Id, Name, get_expiry(), SecretInfo},
  base64:encode(term_to_binary({CookieValue, ?AUTH_SALT, crypto:sha_mac(?AUTH_SECRET, term_to_binary([CookieValue, ?AUTH_SALT]))})).
{% endcodeblock %}</p>

<p>The <code>encode</code> function is rather self-explanatory. We start by encrypting the OAuth token information, we then generate a tuple which includes all the data we want to keep, convert it to binary and <a href="http://en.wikipedia.org/wiki/Base64" title="Base64">base64</a> encode it.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}
decode(CookieValue) ->
  {Value={Id, Name, Expire, SecretInfo}, Salt, Sign} = binary_to_term(base64:decode(CookieValue)),
  case crypto:sha_mac(?AUTH_SECRET, term_to_binary([Value, Salt])) of</p>

<pre><code>Sign -&gt;
  case Expire &gt;= calendar:local_time() of
    true -&gt;
      {Token, TokenSecret} = decrypt(SecretInfo),
      {ok, {Id, Name, Token, TokenSecret}};
    false -&gt;
      {error, expired}
  end;
_ -&gt;
  {error, invalid}
</code></pre>

<p>  end.
{% endcodeblock %}</p>

<p>The <code>decode</code> function does a little more than its counterpart as there's validation built-in as well as decrypting. Firstly we do the inverse of the final steps of the <code>encode</code> function in that we base64 decode the data into binary and convert the resulting binary back to Erlang terms. We then break this value up into its components.</p>

<p>We then validate that the cookie hasn't been tampered with by calculating the <a href="http://en.wikipedia.org/wiki/HMAC" title="HMAC">SHA MAC</a> of the data that was retrieved. If this value doesn't match what is expected we indicate that the value is invalid. If the value is valid, we then make sure that the internal cookie value hasn't expired. If it hasn't, we return <code>{ok, &lt;data&gt;}</code>.</p>

<p>The rest of the functions are easy to understand, so here they are for the sake of completeness without explanation.</p>

<p>{% codeblock apps/csd_web/src/cookie.erl (partial) lang:erlang %}
get_expiry() ->
  {Date, Time} = calendar:local_time(),
  NewDate = calendar:gregorian_days_to_date(calendar:date_to_gregorian_days(Date) + ?AUTH_EXPIRY_DAYS),
  {NewDate, Time}.</p>

<p>encrypt(Value) ->
  crypto:aes_ctr_encrypt(?ENC_KEY, ?ENC_IV, term_to_binary([Value, ?AUTH_SALT])).</p>

<p>decrypt(Value) ->
  [V, ?AUTH_SALT] = binary_to_term(crypto:aes_ctr_decrypt(?ENC_KEY, ?ENC_IV, Value)),
  V.
{% endcodeblock %}</p>

<p>Phew! Now that's out of the way we have some back-end glue which we can use to perform some more interesting tasks. One thing that we really need to do is update the landing page template with something more meaningful than what we have now.</p>

<p>We'll start making use of <a href="http://github.com/evanmiller/erlydtl" title="ErlyDTL">ErlyDTL</a>'s hierarchical templates and implement a base template which our other templates will also make use of. Here it is in all its simplicity:</p>

<p>{% codeblock apps/csd_web/templates/base.dtl lang:html %}
&lt;!DOCTYPE html>
<html lang="en">
  <head></p>

<pre><code>&lt;title&gt;Code Smackdown - {{ "{" }}% block page_title %}{{ "{" }}% endblock %}&lt;/title&gt;
&lt;script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js" type="text/javascript"&gt;&lt;/script&gt;
&lt;script src="http://ajax.microsoft.com/ajax/jquery.templates/beta1/jquery.tmpl.min.js" type="text/javascript"&gt;&lt;/script&gt;
</code></pre>

<p>  </head>
  <body></p>

<pre><code>{{ "{" }}% block body_content %}{{ "{" }}% endblock %}
</code></pre>

<p>  </body>
</html>
{% endcodeblock %}</p>

<p>In this template we've included a couple of Javascript files that we'll be using later on as well as setting up a basic HTML5 page. <code>page_title</code> and <code>body_content</code> are the two sections that child templates can populate with their own content.</p>

<p>With that, let's go ahead and modify our default template so that it has something a little more meaninful in it:</p>

<p>{% codeblock apps/csd_web/templates/home.dtl lang:html %}
{{ "{" }}% extends 'base.dtl' %}</p>

<p>{{ "{" }}% block page_title %}Landing Page{{ "{" }}% endblock %}</p>

<p>{{ "{" }}% block body_content %}</p>

<pre><code>&lt;h1&gt;Welcome to Code Smackdown&lt;/h1&gt;
{{ "{" }}% if logged_in %}
&lt;p&gt;Welcome back {{ "{" }}{ user_name }}.&lt;/p&gt;
{{ "{" }}% else %}
&lt;p&gt;We require you to sign in via Twitter&lt;/p&gt;
&lt;p&gt;&lt;a href="{{ "{" }}{ logon_url }}" title="Sign in with Twitter"&gt;&lt;img src="http://si0.twimg.com/images/dev/buttons/sign-in-with-twitter-d.png"/&gt;&lt;/a&gt;&lt;p&gt;
{{ "{" }}% endif %}
</code></pre>

<p>{{ "{" }}% endblock %}
{% endcodeblock %}</p>

<p>Nothing sinister going on here, but there are a couple of things worth noting. The template now looks for a field called <code>logged_in</code>, and if it's <code>true</code> it renders a paragraph which contains the value in the <code>user_name</code> field. If the <code>logged_in</code> flag is false a link is provided which points to <code>logon_url</code> which ultimately points the user at the Twitter OAuth entry page.</p>

<p>We'll need to pass these values in when we render the template. Let's have a look at the changed section of <code>csd_web_resource</code>:</p>

<p>{% codeblock apps/csd_web/src/csd_web_resource.erl (partial) lang:erlang %}
% ... snip ... %
to_html(ReqData, State) ->
  Content = case cookie:load_auth(ReqData) of</p>

<pre><code>{ok, {_, Name, _, _}} -&gt;
  csd_view:home(Name);
_ -&gt;
  csd_view:home()
</code></pre>

<p>  end,
  {Content, ReqData, State}.
% ... snip ... %
{% endcodeblock %}</p>

<p>Yes this is quite a bit different to before. We are calling into our <code>cookie</code> module to find out if the user is logged in. If they are logged on we call <code>csd_view:home</code> with a single parameter <code>Name</code>, if they're not logged on the same function is called without any parameters.</p>

<p>The <code>csd_view</code> module is new and was created to abstract the idea of template rendering. All the ErlyDTL handling happens in <code>csd_view</code>. Let's take a look at it now.</p>

<p>{% codeblock apps/csd_web/src/csd_view.erl (partial) lang:erlang %}
%% @author OJ Reeves <a href="&#x6d;&#97;&#105;&#x6c;&#116;&#111;&#58;&#111;&#106;&#64;&#98;&#117;&#x66;&#102;&#x65;&#x72;&#101;&#100;&#46;&#x69;&#x6f;">&#x6f;&#x6a;&#x40;&#98;&#x75;&#x66;&#102;&#x65;&#114;&#101;&#100;&#46;&#105;&#111;</a>
%% @copyright 2012 OJ Reeves</p>

<p>-module(csd_view).</p>

<p>-author('OJ Reeves <a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#x6f;&#x3a;&#x6f;&#106;&#x40;&#98;&#x75;&#102;&#102;&#101;&#x72;&#x65;&#x64;&#46;&#105;&#111;">&#x6f;&#106;&#64;&#98;&#117;&#102;&#x66;&#101;&#114;&#x65;&#x64;&#46;&#x69;&#x6f;</a>').</p>

<p>-export([home/0, home/1]).</p>

<p>home() ->
  Params = [{logged_in, false}, {logon_url, conf:get_val(urimap, twitter_logon)}],
  {ok, Content} = home_dtl:render(Params),
  Content.</p>

<p>home(Name) ->
  Params = [{logged_in, true}, {user_name, Name}],
  {ok, Content} = home_dtl:render(Params),
  Content.
{% endcodeblock %}</p>

<p>Here we can see the two functions called <code>home</code> which were invoked in the <code>csd_web_resource</code> module. Most of this module is simple and uninteresting except for the use of <code>conf:get_val</code>. Templates need to know about paths when generating URLs in the markup. In our case, we're rendering links which point to internal routes which are specified in the dispatch list. Rather than hard-code URLs in the templates I decided to create another section in the <code>app.config</code> called <code>urimap</code>. The goal is to have an easy-to-access location for addresses which lives alongside the routes so that the maintainer of the application can update both at the same time should something need to change. Here's what the new section looks like.</p>

<p>{% codeblock apps/csd_web/priv/app.config (partial) lang:erlang %}
% ... snip ... %
  {csd_web,</p>

<pre><code>[
  % ... snip ... %
  {urimap,
    [
      {home, "/"},
      {twitter_logon, "/oauth/request"}
    ]
  },
  % ... snip ... %
</code></pre>

<p>{% endcodeblock %}</p>

<p>Accessing a link address is as simple as running <code>conf:get_val(urimap, &lt;link-id&gt;)</code>.</p>

<p>At this point we can build and run the application to see what the landing page looks like. To fire up the application you'll need three consoles:</p>

<ol>
<li>One for Riak. Riak has to be running behind the scenes because <code>Pooler</code> will connect on start. Only one node is necessary at this point. Run: <code>/path/to/riak/dev/dev1/bin/riak start</code></li>
<li>One for HAProxy. Run: <code>make proxystart</code></li>
<li>One for the CSD application. Run: <code>make webstart</code></li>
</ol>


<p>When you browse to <a href="http://127.0.0.1:4000">http://127.0.0.1:4000</a> you should see the following:</p>

<p><img src="/uploads/2012/02/home-loggedoff.png" title="Home - Logged Off" alt="Landing page when logged off" /></p>

<p>Clicking the link will result in an error at this point, so don't do it yet! We need to implement more resources, but first let's just stick the routes into the dispatch in preparation.</p>

<p>{% codeblock apps/csd_web/priv/app.config (partial) lang:erlang %}
{csd_web,
  [</p>

<pre><code>{web,
  [
    % ... snip ... %
    {dispatch,
      [
        {[], csd_web_resource, []},
        {["snippet", key], csd_web_snippet_resource, []},
        {["oauth", "request"], csd_web_request_resource, []},  % new route
        {["oauth", "callback"], csd_web_callback_resource, []} % new route
      ]
    }
  ]
},
% ... snip ... %
</code></pre>

<p>{% endcodeblock %}</p>

<p>Easily done. Now let's look at the implementation of the first handler which handles the <code>/oauth/request</code> URI, <code>csd_web_request_resource</code>.</p>

<p>{% codeblock apps/csd_web/src/csd_web_request_resource.erl lang:erlang %}
%% @author OJ Reeves <a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#111;&#x3a;&#111;&#106;&#64;&#98;&#x75;&#x66;&#102;&#x65;&#114;&#101;&#100;&#46;&#105;&#x6f;">&#111;&#106;&#64;&#98;&#117;&#x66;&#102;&#101;&#114;&#x65;&#100;&#x2e;&#x69;&#x6f;</a>
%% @copyright 2012 OJ Reeves</p>

<p>-module(csd_web_request_resource).</p>

<p>-author('OJ Reeves <a href="&#109;&#x61;&#105;&#108;&#x74;&#111;&#x3a;&#111;&#106;&#x40;&#x62;&#117;&#x66;&#102;&#101;&#x72;&#x65;&#100;&#x2e;&#x69;&#111;">&#x6f;&#106;&#64;&#x62;&#117;&#x66;&#x66;&#x65;&#x72;&#101;&#x64;&#46;&#x69;&#111;</a>').</p>

<p>-export([init/1, resource_exists/2, previously_existed/2, moved_temporarily/2]).</p>

<p>-include_lib("webmachine/include/webmachine.hrl").</p>

<p>init([]) ->
  {ok, undefined}.</p>

<p>resource_exists(ReqData, State) ->
  {false, ReqData, State}.</p>

<p>previously_existed(ReqData, State) ->
  {true, ReqData, State}.</p>

<p>moved_temporarily(ReqData, State) ->
  {ok, Url} = twitter:request_access(),
  {{ "{" }}{true, Url}, ReqData, State}.
{% endcodeblock %}</p>

<p>This module is quite lightweight, but has a little bit of magic in it that revolves around getting redirects to work. If you're not familiar with how 307 redirects work in Webmachine, take a quick side-glance at my <a href="http://buffered.io/posts/redirects-with-webmachine/" title="Redirects with Webmachine">Redirects with Webmachine</a> post.</p>

<p>Back? Ok. The extra line of code in the <code>moved_temporarily</code> function is where we invoke <code>twitter:request_access()</code> which goes to Twitter.com and gets a request token. The URL generated by this call is then passed back to Webmachine which will tell the caller's browser where to redirect to.</p>

<p>Build the app, fire up it up and click on the "Sign in via Twitter" button and you should see a screen that resembles this (assuming you're already signed in to Twitter):</p>

<p><img src="/uploads/2012/02/twitter-logon.png" title="Twitter - Logon Page" alt="Logging into CSD view Twitter" /></p>

<p>Exciting! We're nearly there. Don't click "Sign In" just yet because we don't yet have the callback set up to handle the result. Let's do that now. Here's the resource:</p>

<p>{% codeblock apps/csd_web/src/csd_web_callback_resource.erl lang:erlang %}
%% @author OJ Reeves <a href="&#109;&#x61;&#x69;&#108;&#x74;&#111;&#58;&#x6f;&#x6a;&#64;&#x62;&#117;&#x66;&#x66;&#x65;&#114;&#101;&#100;&#46;&#105;&#x6f;">&#111;&#106;&#x40;&#98;&#x75;&#x66;&#x66;&#x65;&#114;&#x65;&#x64;&#46;&#105;&#111;</a>
%% @copyright 2012 OJ Reeves</p>

<p>-module(csd_web_callback_resource).</p>

<p>-author('OJ Reeves <a href="&#x6d;&#97;&#105;&#x6c;&#x74;&#111;&#58;&#x6f;&#x6a;&#64;&#98;&#x75;&#102;&#102;&#x65;&#114;&#101;&#x64;&#46;&#x69;&#x6f;">&#x6f;&#106;&#x40;&#x62;&#x75;&#102;&#102;&#x65;&#x72;&#101;&#100;&#x2e;&#105;&#x6f;</a>').</p>

<p>-export([init/1, resource_exists/2, previously_existed/2, moved_temporarily/2]).</p>

<p>-include_lib("webmachine/include/webmachine.hrl").</p>

<p>init([]) ->
  {ok, undefined}.</p>

<p>resource_exists(ReqData, State) ->
  {false, ReqData, State}.</p>

<p>previously_existed(ReqData, State) ->
  {true, ReqData, State}.</p>

<p>moved_temporarily(ReqData, State) ->
  handle_callback(ReqData, State).</p>

<p>handle_callback(ReqData, State) ->
  ReqToken = wrq:get_qs_value("oauth_token", ReqData),
  ReqTokenSecret = wrq:get_qs_value("oauth_token_secret", ReqData),
  Verifier = wrq:get_qs_value("oauth_verifier", ReqData),</p>

<p>  {ok, AccessToken, AccessTokenSecret} = twitter:verify_access(ReqToken, ReqTokenSecret, Verifier),
  {ok, UserInfoJson} = twitter:get_current_user_info(AccessToken, AccessTokenSecret),
  {struct, Json} = mochijson2:decode(UserInfoJson),
  UserId = proplists:get_value(&lt;&lt;"id">>, Json),
  UserName = proplists:get_value(&lt;&lt;"screen_name">>, Json),
  NewReqData = cookie:store_auth(ReqData, UserId, UserName, AccessToken, AccessTokenSecret),</p>

<p>  % TODO: store the 'session' in Riak in an ETS backend</p>

<p>  % TODO: error handlng for when things don't go to plan
  {{ "{" }}{true, conf:get_val(urimap, home)}, NewReqData, State}.</p>

<p>{% endcodeblock %}</p>

<p>The first few parts of this module should look familiar by now. We are overriding the <code>resource_exists</code>, <code>previously_existed</code> and <code>moved_temporarility</code> functions because we're going to be redirecting. For now we're going to assume that the user clicked "Sign In" and that everything went according to plan. Later on we'll worry about handling logon errors.</p>

<p>When <code>moved_temporarily</code> is invoked we pass responsibility off to the <code>handle_callback</code> function. Here you can see we are taking three parameters out of the query string that Twitter sent through to us. Those parameters are the <em>request token</em>, <em>request token secret</em> and the <em>verifier</em>. We take those values and pass them down to our <code>twittter</code> module to get it to verify the access with Twitter and to generate an <em>access token/access token secret</em> pair. When that comes back we have our token information and we can assume that the user has authenticated via Twitter. At this point we can "log the user on" by storing a cookie, but before we do that we want to get their Twitter ID and Username, so we invoke the <code>twitter:get_current_user_info</code> function, passing in the OAuth credentials, which in return gives us a blob of <a href="http://json.org/" title="JavaScript Object Notation">JSON</a> which contains the user's Twitter information.</p>

<p>From that we glean their ID and Username. We then store that information, along with the access token information, in a cookie using <code>cookie:store_ath</code> (which we've covered previously) and we get a new request data object out as a result.</p>

<p>Now all we have to do is redirect the user back to the home page and pass on the new request data. Webmachine will take this data and push the cookie to the user's browser, then redirect the user to the <code>home</code> entry in the <code>urimap</code> section in <code>app.config</code>. In effect, we're redirected to the home page as a logged on user.</p>

<p>Ignoring the <code>TODO</code> notes (which we'll cover in future posts in this series), we've got ourselves to the point where the application should function end-to-end. Finally.</p>

<p>Compile the application and fire it up! Let's take a look at what happens.</p>

<p><img src="/uploads/2012/02/home-loggedoff.png" title="Home - Logged Off" alt="Hitting the home page prior to logging on" /></p>

<p><img src="/uploads/2012/02/twitter-logon.png" title="Twitter - Logon Page" alt="Authenticating with Twitter" /></p>

<p><img src="/uploads/2012/02/home-loggedon.png" title="Home - Logged On" alt="Back home after the redirect with successful sign-on" /></p>

<h2>That's all ... for now</h2>

<p>Thanks for reading this post. If you managed to make it this far you've done well. In the next post we'll start to do some more meaningful things with our logged on users, such as allowing them to submit code snippets. This is where the end-to-end process becomes interesting.</p>

<p>Comments, feedback and criticisms are as welcome as always.</p>

<p><strong>Note:</strong> The code for Part 4 (this post) can be found on <a href="https://github.com/OJ/csd/tree/Part4-20120217" title="Source code for Part 4">Github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Webmachine, ErlyDTL and Riak - Part 3]]></title>
    <link href="http://buffered.io/posts/webmachine-erlydtl-and-riak-part-3/"/>
    <updated>2010-10-13T06:31:00+10:00</updated>
    <id>http://buffered.io/posts/webmachine-erlydtl-and-riak-part-3</id>
    <content type="html"><![CDATA[<p><img src="http://buffered.io/uploads/2010/09/riak-logo.png" alt="Riak Logo" style="float:left;padding-right:5px;padding-bottom:5px;"/>For those of you who are new to the series, you may want to check out <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a> and <a href="/posts/webmachine-erlydtl-and-riak-part-2/" title="Wembachine, ErlyDTL and Riak - Part 2">Part 2</a> before reading this post. It will help give you some context as well as introduce you to some of the jargon and technology that I'm using. If you've already read then, or don't want to, then please read on!</p>

<p>This post builds on the previous two, but not without a few little modifications. If you're interested in following along step by step with your own version of the code running, then get yourself a copy of <a href="https://bitbucket.org/OJ/csd/changeset/df62880d12a8" title="Source code for Part 2">this changeset</a> before doing so.</p>

<p>In this post we're going to cover:</p>

<ol>
<li>A slight refactor of code structure to support the "standard" approach to building applications in Erlang using OTP.</li>
<li>Building a small set of modules to talk to <a href="http://www.basho.com/developers.html#Riak" title="Riak">Riak</a>.</li>
<li>Creation of some <a href="http://json.org/" title="JavaScript Object Notation">JSON</a> helper functions for reading and writing data.</li>
<li>Calling all the way from the <a href="http://www.basho.com/developers.html#Webmachine" title="Webmachine">Webmachine</a> front-end to Riak to extract data and display it in a browser using <a href="http://github.com/evanmiller/erlydtl" title="ErlyDTL">ErlyDTL</a> templates.</li>
</ol>


<p>There are quite a few code snippets in this post as well as output from script executions and <code>bash</code> sessions. To avoid confusion, all file listings reference the path to the file that is being modified relative to the root of the project folder.</p>

<p>Be warned, this is a <em>long</em> post :) Get yourself a <em>shmoke und a pancake</em>, a glass of your favourite beverage and put some relaxing music on (instrumental is best).</p>

<p>Are you ready? OK, here we go ...</p>

<!--more-->


<h2>A Slight Refactor</h2>

<p>I was ready to embark on this third post a while back but then I sat back and thought about how I might structure things if I were using another set of technologies. Usually I would put another layer between the web tier and the back-end database cluster as opposed to having the web tier talk to the database directly. It didn't make sense to me that this approach would be any different in Erlang.</p>

<p>I had a chat to <a href="http://twitter.com/sj_mackenzie" title="Stewart Mackenzie on Twitter">two</a> <a href="http://twitter.com/MatthewErbs" title="Matt Erbs on Twitter">blokes</a> that I really respect to get their views, and then I fired off a question to the Basho guys (via the <a href="irc://irc.freenode.com/riak" title="Riak IRC on Freenode">#riak IRC channel</a>). The Basho lads even made the effort to respond to me via the <a href="http://lists.basho.com/pipermail/riak-users_lists.basho.com/2010-September/001984.html" title="Riak Recap">Riak Recap</a> as they weren't available at the time to answer me via IRC (thanks again <a href="http://twitter.com/pharkmillups" title="Mark Phillips on Twitter">Mark</a>). All three of them confirmed my thoughts. Here's what appeared in the recap which captures the question and response nicely:</p>

<blockquote><p>Q --- I have a Webmachine application which will be talking to Riak. I was going to put application and controller logic in that application and I am wondering if [I] should instead be creating a "core" OTP application with the business style logic in it and have the Webmachine app talk to that app which, in turn, talks to Riak? Is that the general approach that is taken [in Erlang applications]? (from TheColonial via #riak)</p>

<p>A --- We recommend going with the latter approach. You're better off to create a core app that talks to Webmachine and Riak separately.</p></blockquote>

<p>Perfect, that makes total sense. Therefore the following describes what I did to modify the code base that I had in order to support this set up. <strong>Any failure</strong> in implementation, structure or understanding is totally my own and in no way reflects on the abilities and advice of those mentioned above who took the time to offer assistance.</p>

<p>Moving on. What we want to end up with is three applications:</p>

<table cellspacing="0">
  <thead>
    <tr>
      <th style="text-align:center;">Application</th>
      <th style="text-align:center;">Structure/Responsibility</th>
      <th style="text-align:center;">Talks to</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Riak</td>
      <td>Bomb-proof data storage and replication.</td>
      <td style="text-align:center">-</td>
    </tr>
    <tr>
      <td>csd_core</td>
      <td>An OTP application that provides an API to a set of logic that deals with the transformation of data from a client through to the database. It should keep the clients ignorant of the data storage medium. It should provide business logic that would be required for any client application to be able to talk to a `csd`/Riak back-end.</td>
      <td style="text-align:center">Riak</td>
    </tr>
    <tr>
      <td>csd_web</td>
      <td>Provide a nice, web-based interface for the user to experience the goal of the Code Smackdown application.</td>
      <td style="text-align:center">csd_core</td>
    </tr>
  </tbody>
</table>


<p>Given that we're going to be using this structure, the "root" folder should actually be fairly clean without any source. Instead, each <code>csd</code>-related application should live in its own sub-folder under an <code>apps</code> folder and the root should just contain the means to build it and start it. In essence what we'd like to see in the root folder is something like this:</p>

<pre><code>oj@spawn-link ~/blog/csd $ ls -F
apps/  dev.haproxy.conf  Makefile  proxy.sh*  rebar*  rebar.config  start.sh*
</code></pre>

<p>With that in mind, let's start the surgery.</p>

<h3>Moving csd to csd_web</h3>

<p>There are two ways to approach this problem. The first is to do a <strong>find and replace</strong>, making sure you cover off file names as well as module names, etc. The second is to simply <strong>recreate the web site from scratch</strong>, copy over any missing files and make any other adjustments manually that may be required.</p>

<p>I preferred the second approach, so that's what I did. First I recreated the web application, which is now called <code>csd_web</code> in the <code>apps</code> folder:</p>

<pre><code>oj@spawn-link ~/blog/csd $ mkdir apps &amp;&amp; cd apps
oj@spawn-link ~/blog/csd/apps $ ~/blog/webmachine/scripts/new_webmachine.sh csd_web .
==&gt; priv (create)
Writing /home/oj/blog/csd/apps/csd_web/README
Writing /home/oj/blog/csd/apps/csd_web/Makefile
Writing /home/oj/blog/csd/apps/csd_web/rebar.config
Writing /home/oj/blog/csd/apps/csd_web/rebar
Writing /home/oj/blog/csd/apps/csd_web/start.sh
Writing /home/oj/blog/csd/apps/csd_web/ebin/csd_web.app
Writing /home/oj/blog/csd/apps/csd_web/src/csd_web.erl
Writing /home/oj/blog/csd/apps/csd_web/src/csd_web_app.erl
Writing /home/oj/blog/csd/apps/csd_web/src/csd_web_sup.erl
Writing /home/oj/blog/csd/apps/csd_web/src/csd_web_resource.erl
Writing /home/oj/blog/csd/apps/csd_web/priv/dispatch.conf
oj@spawn-link ~/blog/csd/apps $ ls -F
csd_web/
</code></pre>

<p>Next I removed a few files which weren't going to be needed any more. I then copied over <code>rebar.config</code>, the ErlyDTL templates and the <code>csd.app.src</code> file (which we need to modify):</p>

<pre><code>oj@spawn-link ~/blog/csd/apps $ cd csd_web
oj@spawn-link ~/blog/csd/apps/csd_web $ rm README rebar start.sh
oj@spawn-link ~/blog/csd/apps/csd_web $ cp ../../rebar.config .
oj@spawn-link ~/blog/csd/apps/csd_web $ cp -R ../../templates .
oj@spawn-link ~/blog/csd/apps/csd_web $ cp ../../src/csd.app.src ./src/csd_web.app.src
</code></pre>

<p>I then edited the <code>csd_web.app.src</code> file so that the names were updated (I tidied it up a little and added a version number too):</p>

<p>{% codeblock apps/csd_web/src/csd_web.app.src lang:erlang %}
%%-<em>- mode: erlang -</em>-
{application, csd_web,
  [</p>

<pre><code>{description, "The Webmachine component of the Code Smackdown application."},
{vsn, "0.0.1"},
{modules, []},
{registered, []},
{applications,
  [
    kernel,
    stdlib,
    crypto,
    mochiweb,
    webmachine
  ]
},
{mod, {csd_web_app, []}},
{env, []}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>I then opened up <code>csd_web_resource.erl</code> and made it look like the original <code>csd_resource.erl</code> so that it called the ErlyDTL template:</p>

<p>{% codeblock apps/csd_web/src/csd_web_resource.erl lang:erlang %}
-module(csd_web_resource).
-export([init/1, to_html/2]).</p>

<p>-include_lib("webmachine/include/webmachine.hrl").</p>

<p>init([]) ->
  {ok, undefined}.</p>

<p>to_html(ReqData, State) ->
  {ok, Content} = sample_dtl:render([{param, "Slartibartfast"}]),
  {Content, ReqData, State}.
{% endcodeblock %}</p>

<p><code>csd_web</code> is now ready to go. To build it, we need to go back up to the root <code>csd</code> folder and adjust the <code>rebar.config</code> so that it knows to look in the <code>apps</code> sub-folder (thanks to <a href="http://twitter.com/andrewtj" title="AndrewTJ on Twitter">Andrew</a> for <a href="http://lists.basho.com/pipermail/rebar_lists.basho.com/2010-October/000246.html" title="Configuring the Rebar apps folder on Basho list">pointing this out</a>). We can also remove all the dependencies because that will be taken care of by <code>csd_web</code>:</p>

<p>{% codeblock rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{sub_dirs, ["apps/csd_web"]}.
{% endcodeblock %}</p>

<p>Next, I removed all the other left-over stuff in the root folder that wasn't required any more (including the startup script):</p>

<pre><code>oj@spawn-link ~/blog/csd $ rm -rf README priv src templates start.sh
</code></pre>

<p>I then modify the <code>Makefile</code> so that it does a couple of other things:</p>

<ol>
<li>Includes a target which builds just the current applications <em>without</em> building the dependencies (this will make builds much quicker most of the time).</li>
<li>Includes a target which can start the web application, essentially replacing the original startup script. This target will be dependent on the previous target so that it is always up to date when running the application.</li>
<li>Includes targets which can start/stop <code>HAproxy</code>.</li>
</ol>


<p>{% codeblock Makefile %}
ERL ?= erl
APP = csd</p>

<p>.PHONY: deps</p>

<p>all: deps
  @./rebar compile</p>

<p>app:
  @./rebar compile skip_deps=true</p>

<p>deps:
  @./rebar get-deps</p>

<p>clean:
  @./rebar clean</p>

<p>distclean: clean
  @./rebar delete-deps</p>

<p>webstart: app
  exec erl -pa $(PWD)/apps/*/ebin -pa $(PWD)/deps/*/ebin -boot start_sasl -s reloader -s csd_web</p>

<p>proxystart:
  @haproxy -f dev.haproxy.conf
{% endcodeblock %}</p>

<p>All that is left to do is start <code>haproxy</code> and launch the application (make sure <code>Riak</code> is running first). These commands need to be done in two different terminal windows. First, start the proxy (note the use of <code>sudo</code> so that we can listen on port 80):</p>

<pre><code>oj@spawn-link ~/blog/csd $ sudo make proxystart
[2] 1935
Available polling systems :
     sepoll : pref=400,  test result OK
      epoll : pref=300,  test result OK
       poll : pref=200,  test result OK
     select : pref=150,  test result OK
Total: 4 (4 usable), will use sepoll.
Using sepoll() as the polling mechanism.
</code></pre>

<p>Then make and start the web application. We have to do a full <code>make</code> first time around so that all the dependencies are resolved:</p>

<pre><code>oj@spawn-link ~/blog/csd $ make &amp;&amp; make webstart

   ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::21:04:18 ===
         application: csd_web
          started_at: nonode@nohost
</code></pre>

<p>Now we should be able to hit <a href="http://localhost/" title="localhost web app">localhost</a> and see the ErlyDTL template rendered in all its awesome, black-and-white glory:</p>

<p><img src="http://buffered.io/uploads/2010/10/localhost-slartibartfast.png" /></p>

<p>Refactor complete. Now let's start work on our new OTP application which will be responsible for talking to Riak.</p>

<p>If you need a break, now is the time to take it! Go freshen up, take a leak and refill your glass.</p>

<p>Ready to go again? Here we go ...</p>

<h3>Creating the csd_core OTP Application</h3>

<p>Creation of an OTP-compliant application is another job for <a href="http://www.basho.com/developers.html#Rebar" title="Rebar">Rebar</a> as it comes with a set of templates built-in. Unfortunately those template aren't 100% and hence don't do everything we need to do out of the box. But we shall use them as a starting point:</p>

<pre><code>oj@spawn-link ~/blog/csd $ mkdir apps/csd_core &amp;&amp; cd apps/csd_core
oj@spawn-link ~/blog/csd/apps/csd_core $ ../../rebar create-app appid=csd_core
==&gt; csd_core (create-app)
Writing src/csd_core.app.src
Writing src/csd_core_app.erl
Writing src/csd_core_sup.erl
</code></pre>

<p>We have a very simple application shell set up, but we need to do a bit more work to get it ready. First, let's create our base <code>csd_core.erl</code> module which is used to fire up our application. For this we will use <code>csd_web.erl</code> (the one which is part of our Webmachine application) as a template. Note that I've shuffled things around and removed some things that are not relevant:</p>

<p>{% codeblock apps/csd_core/src/csd_core.erl lang:erlang %}
%% @author OJ Reeves <a href="&#x6d;&#97;&#105;&#108;&#x74;&#111;&#x3a;&#111;&#106;&#x40;&#x62;&#117;&#x66;&#x66;&#x65;&#114;&#101;&#100;&#x2e;&#x69;&#x6f;">&#111;&#106;&#64;&#x62;&#x75;&#102;&#x66;&#101;&#x72;&#x65;&#x64;&#x2e;&#x69;&#111;</a>
%% @copyright 2011 OJ Reeves</p>

<p>%% @doc csd_core startup code</p>

<p>-module(csd_core).
-author('OJ Reeves <a href="&#x6d;&#x61;&#x69;&#x6c;&#x74;&#111;&#x3a;&#x6f;&#106;&#64;&#98;&#117;&#102;&#x66;&#x65;&#114;&#101;&#x64;&#46;&#105;&#111;">&#x6f;&#x6a;&#64;&#x62;&#117;&#x66;&#x66;&#x65;&#114;&#x65;&#100;&#x2e;&#105;&#111;</a>').
-export([start/0, start_link/0, stop/0]).</p>

<p>ensure_started(App) -></p>

<pre><code>case application:start(App) of
    ok -&gt;
        ok;
    {error, {already_started, App}} -&gt;
        ok
end.
</code></pre>

<p>%% @spec start_link() -> {ok,Pid::pid()}
%% @doc Starts the app for inclusion in a supervisor tree
start_link() -></p>

<pre><code>ensure_started(crypto),
csd_core_sup:start_link().
</code></pre>

<p>%% @spec start() -> ok
%% @doc Start the csd_core server.
start() -></p>

<pre><code>ensure_started(crypto),
application:start(csd_core).
</code></pre>

<p>%% @spec stop() -> ok
%% @doc Stop the csd_core server.
stop() -></p>

<pre><code>Res = application:stop(csd_core),
application:stop(crypto),
Res.
</code></pre>

<p>{% endcodeblock %}</p>

<p>Next up, edit <code>csd_core.app.src</code> and add some application-specific information:</p>

<p>{% codeblock apps/csd_core/src/csd_core.app.src lang:erlang %}</p>

<p>{application, csd_core,
  [</p>

<pre><code>{description, "Core functionality for the Code Smackdown application."},
{vsn, "0.0.1"},
{registered, []},
{applications,
  [
    kernel,
    stdlib
  ]
},
{mod, {csd_core_app, []}},
{env, []}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>We know that we'll be talking to Riak, so we need to make sure we've included the <code>riakc</code> (Riak client) dependency. Though I haven't yet talked about it, we'll also be using Mochiweb's <a href="https://github.com/mochi/mochiweb/blob/master/src/mochijson2.erl" title="Mochiweb's json module">mochijson2</a> module to help with handling JSON data, so we shall add this as a dependency to the application. Bear in mind this is already a dependency for the web component of the application, so we're not actually adding a <em>new</em> dependency to the overall application.</p>

<p>We can do this by creating a <code>rebar.config</code> in <code>apps/csd_core</code> and editing it to contain the following:</p>

<p>{% codeblock apps/csd_core/rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{deps,
  [</p>

<pre><code>{mochiweb, "1.5.1", {git, "git://github.com/mochi/mochiweb", {tag, "1.5.1"}}},
{riakc, ".*", {git, "git://github.com/basho/riak-erlang-client", "HEAD"}}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>Then we need to tell <code>rebar</code> to build this new application by adjusting the <code>rebar.config</code> in the <code>csd</code> root folder:</p>

<p>{% codeblock rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{sub_dirs, ["apps/csd_core", "apps/csd_web"]}.
{% endcodeblock %}</p>

<p>Now we have enough to get the <code>csd_core</code> application started, even though it doesn't do anything. We just need to adjust our <code>Makefile</code> target so that it launches the <code>csd_core</code> application as well:</p>

<p>{% codeblock Makefile %}
ERL ?= erl
APP = csd</p>

<p>.PHONY: deps</p>

<p>all: deps
  @./rebar compile</p>

<p>app:
  @./rebar compile skip_deps=true</p>

<p>deps:
  @./rebar get-deps</p>

<p>clean:
  @./rebar clean</p>

<p>distclean: clean
  @./rebar delete-deps</p>

<p>webstart: app
  exec erl -pa $(PWD)/apps/*/ebin -pa $(PWD)/deps/*/ebin -boot start_sasl -s reloader -s csd_core -s csd_web</p>

<p>proxystart:
  @haproxy -f dev.haproxy.conf
{% endcodeblock %}</p>

<p>Then off we go:</p>

<pre><code>oj@spawn-link ~/blog/csd $ make webstart
==&gt; csd_core (compile)
Compiled src/csd_core_app.erl
Compiled src/csd_core_sup.erl
Compiled src/csd_core.erl

   ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::21:49:27 ===
         application: csd_core
          started_at: nonode@nohost

   ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::21:49:27 ===
         application: csd_web
          started_at: nonode@nohost
</code></pre>

<p>As you can see we now have a system which contains both <code>csd_core</code> and <code>csd_web</code>. This is great, but <code>csd_core</code> needs a lot more work. The intent for this application is to be an <a href="http://en.wikipedia.org/wiki/Open_Telecom_Platform" title="Open Telecom Platform">OTP</a> application which provides an API to the <code>csd</code> logic and back-end database. This means we're going to need to get ourselves a <a href="http://www.erlang.org/doc/design_principles/gen_server_concepts.html" title="gen_server behaviour">gen_server</a> set up which can handle requests from various clients. Let's do that next.</p>

<p>Thankfully <code>rebar</code> comes with a simple template that we can use for creating the <code>gen_server</code> behaviour, so we can invoke that from the command line and have it generate the shell for us:</p>

<pre><code>oj@spawn-link ~/blog/csd/apps/csd_core $ ../../rebar create template=simplesrv srvid=csd_core_server
==&gt; csd_core (create)
Writing src/csd_core_server.erl
</code></pre>

<p>We now have a very dumb server ready to go, to make it start with the rest of the application we have to modify <code>csd_core_sup</code>, the <a href="http://www.erlang.org/doc/design_principles/sup_princ.html" title="supervisor behaviour">supervisor</a> and tell it to fire up the server for us:</p>

<p>{% codeblock apps/csd_core/src/csd_core_sup.erl lang:erlang %}
-module(csd_core_sup).</p>

<p>-behaviour(supervisor).</p>

<p>%% API
-export([start_link/0]).</p>

<p>%% Supervisor callbacks
-export([init/1]).</p>

<p>%% Helper macro for declaring children of supervisor
-define(CHILD(I, Type), {I, {I, start_link, []}, permanent, 5000, Type, [I]}).</p>

<p>%% ===================================================================
%% API functions
%% ===================================================================</p>

<p>start_link() ->
  supervisor:start_link({local, ?MODULE}, ?MODULE, []).</p>

<p>%% ===================================================================
%% Supervisor callbacks
%% ===================================================================</p>

<p>init([]) ->
  Server = ?CHILD(csd_core_server, worker),
  Processes = [Server],
  {ok, { {one_for_one, 5, 10}, Processes} }.
{% endcodeblock %}</p>

<p>With this in place we can now start our application again and we should see the new <code>csd_core_server</code> appear in the start-up sequence:</p>

<pre><code>oj@spawn-link ~/blog/csd $ make webstart

   ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::22:04:04 ===
          supervisor: {local,csd_core_sup}
             started: [{pid,&lt;0.54.0&gt;},
                       {name,csd_core_server},
                       {mfargs,{csd_core_server,start_link,[]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

=PROGRESS REPORT==== 4-Apr-2011::22:04:04 ===
         application: csd_core
          started_at: nonode@nohost

   ... snip ...
</code></pre>

<p>The shell and structure of our application is now in place. We are finally ready to start talking to Riak!</p>

<p>Again, now is the time to have a mini-break if you need one. Grab a <em>Shigar und a waffle</em> and a cup of English Breakfast tea.</p>

<h2>Preparing csd_core for Riak connectivity</h2>

<p>Given that this is the first look at connecting to Riak, we're going to have to set up a little infrastructure to support our needs. As a result, the data itself won't be discussed much for fear of turning this post into something way more epic than originally intended.</p>

<p>So in short, we're interested in storing the idea of a <em>code snippet</em>. That is an entity which contains two opposing blobs of code which are being compared. That snippet will have a title. Down the track, more information will be associated with this snippet, such as the author, along with links to a set of comments and votes. For now we'll just focus on storing the bare essentials of the snippet.</p>

<h3>The Snippet</h3>

<p>As far as our Erlang code is concerned, our snippet is going to be a simple list of properties that we can interact with via the <a href="http://www.erlang.org/doc/man/proplists.html" title="proplists">proplists</a> module. This keeps things really simple. To demonstrate what our snippet will look like in code, here is the function that takes a Title, and the two code blobs (called Left and Right) and returns a <code>snippet</code> instance. This code goes in a module called <code>csd_snippet</code> defined in <code>src/csd_snippet.erl</code>:</p>

<p>{% codeblock apps/csd_core/src/csd_snippet.erl (part) lang:erlang %}
to_snippet(Title, Left, Right) ->
  {snippet,</p>

<pre><code>[
  {title, Title},
  {left, Left},
  {right, Right}
]
</code></pre>

<p>  }.
{% endcodeblock %}</p>

<p>Note that the first part of the tuple is the atom <code>snippet</code> which I am using to identify the layout of the contents in the second part of the tuple. Down the track we'll have more collections of data in the system than just snippets, and we may want to make sure that the caller doesn't accidentally pass in a <code>user</code>, for example, to a function expecting a <code>snippet</code>.</p>

<p>It is important at this point to note that, down the track, I will include a <code>key</code> property in all of the data objects that are pushed to Riak. This property serves as the identifier for the object in Riak and is stored alongside the rest of the data so that it is easy to relate the in-memory instance back to the stored instance. This value, if not specified, will be inserted automatically when an item is saved via the API functions in <code>csd_core</code>. More on this later.</p>

<h3>Formatting Data for Storage in Riak</h3>

<p>Riak is very flexible in that it will store whatever kind of information you give it. This is good because it means we can cater our data format to whatever needs we have.</p>

<p>In our case, the <em>easiest</em> option would be to store our Erlang terms as binary using <a href="http://www.erlang.org/doc/man/erlang.html#term_to_binary-1" title="term_to_binary">term_to_binary</a> as we wouldn't have to think about <em>anything</em> else. We could easily read the data using <a href="http://www.erlang.org/doc/man/erlang.html#binary_to_term-1" title="binary_to_term">binary_to_term</a>. Done.</p>

<p>This comes with a set of problems though. For example, if we wanted to <a href="http://en.wikipedia.org/wiki/MapReduce" title="map/reduce">map/reduce</a> using JavaScript we wouldn't find it easy to get the data into a format that we could use. Another example would be that the RESTful interface to Riak would be close to useless because <strong>any</strong> non-Erlang client would have to somehow get the data into a meaningful format to work with.</p>

<p>Instead of using binary and throwing Erlang terms straight into Riak, we're going to use <a href="http://json.org/" title="JavaScript Object Notation">JSON</a>. It's very easy to convert to and from JSON in many different languages, and it's very easy to read. We can also easily verify that the data is being stored correctly by querying Riak's RESTful interface directly using <a href="http://curl.haxx.se/" title="cURL homepage">cURL</a> or a browser.</p>

<p>In order to store data in JSON format, we're going to enlist the help of <a href="https://github.com/mochi/mochiweb/blob/master/src/mochijson2.erl" title="Mochiweb's json module">mochijson2</a>, a library that comes with <a href="https://github.com/mochi/mochiweb" title="Mochiweb">Mochiweb</a> that makes it a <em>lot</em> easier to deal with JSON than doing everything manually. Given that we're using Webmachine for the front-end (which itself relies on Mochiweb) we already have the dependency available.</p>

<p>Unfortunately we can't just throw our data straight at this module and have it do everything for us. <code>mochijson2</code> requires data to be in a certain format before it can encode it to JSON. When decoding <em>from</em> JSON, it converts the data into the same format. Hence, we need the ability to convert our own data format to and from this intermediate data format so that <code>mochijson2</code> can deal with it.</p>

<p>We need two functions: <code>to_json()</code> and <code>from_json()</code>, and we shall define these in a helper module called <code>csd_json</code>. This module will live in <code>csd_core</code>:</p>

<p>{% codeblock apps/csd_core/src/csd_json.erl (part) lang:erlang %}
-module(csd_json).
-export([from_json/1, from_json/2, to_json/1, to_json/2]).</p>

<p>to_json(PropList) ->
  to_json(PropList, fun(_) -> true end).</p>

<p>to_json(PropList, IsStrFun) ->
  list_to_binary(mochijson2:encode(from_proplist(PropList, IsStrFun))).</p>

<p>from_json(Json) ->
  from_json(Json, fun(_) -> true end).</p>

<p>from_json(Json, IsStrFun) ->
  to_proplist(mochijson2:decode(Json), IsStrFun).
{% endcodeblock %}</p>

<p>You're probably wondering why each of these functions requires the <code>IsStrFun</code> parameter (if you're not, you're obviously an experienced Erlanger!). For those who don't know, strings in Erlang are actually lists of integers. This is fantastic as it makes it easy to manipulate strings as if they were lists, but it comes at a small price: it's not possible to determine the difference between a list of integers and a string.</p>

<p>Why is this important? <code>mochijson2</code> needs strings to be encoded as binaries, so we need a way to differentiate between integer lists and real strings. My original implementations of both the <code>to_json()</code> and <code>from_json()</code> functions attempted to figure out if certain fields were strings or not by looking at the content of the list. Not only was the code messy, but it wasn't foolproof. Instead, I made the decision to force the user to provide a callback function which will tell the JSON serialiser if the given property is a string or not. This callback takes a single parameter which is the name (in atom form) of the property and returns a boolean -- <code>true</code> indicates that the value is a string, <code>false</code> otherwise.</p>

<p>In some cases we might just be happy to encode/decode every single value as a string. Hence, there is an overload to both <code>to_json()</code> and <code>from_json()</code> which caters for this case. The rest of the code which implments the conversion is listed below. Don't feel that you need to understand the code below, as it's really not the goal of this post. The full source to this module is included in the source link specified at the end of this post.</p>

<p>{% codeblock apps/csd_core/src/csd_json.erl (part) lang:erlang %}
from_proplist(List=[H|<em>], IsStrFun) when is_tuple(H) ->
  { struct, lists:map(fun(P) -> from_proplist(P, IsStrFun) end, List) };
from_proplist({PropName, ComplexProp=[H|</em>]}, IsStrFun) when is_tuple(H) ->
  { list_to_binary(atom_to_list(PropName)), from_proplist(ComplexProp, IsStrFun) };
from_proplist({PropName, PropVal}, IsStrFun) ->
  { list_to_binary(atom_to_list(PropName)), to_value(PropName, PropVal, IsStrFun) }.</p>

<p>to_proplist({struct, PropList}, IsStrFun) when is_list(PropList) ->
  lists:map(fun(P) -> to_proplist(P, IsStrFun) end, PropList);
to_proplist({PropName, ComplexProp={struct, _}}, IsStrFun) ->
  { list_to_atom(binary_to_list(PropName)), to_proplist(ComplexProp, IsStrFun) };
to_proplist({PropName, PropVal}, IsStrFun) ->
  PropAtom = list_to_atom(binary_to_list(PropName)),
  { PropAtom, from_value(PropAtom, PropVal, IsStrFun) }.</p>

<p>to_value(PropName, L=[H|_], IsStrFun) when is_list(L) and is_list(H) ->
  lists:map(fun(P) -> to_value(PropName, P, IsStrFun) end, L);
to_value(PropName, L, IsStrFun) when is_list(L) ->
  case IsStrFun(PropName) of</p>

<pre><code>true -&gt; list_to_binary(L);
_ -&gt; lists:map(fun(V) -&gt; to_value(PropName, V, IsStrFun) end, L)
</code></pre>

<p>  end;
to_value(<em>, V, </em>) ->
  V.</p>

<p>from_value(PropName, L, IsStrFun) when is_list(L) ->
  lists:map(fun(P) -> from_value(PropName, P, IsStrFun) end, L);
from_value(PropName, B, IsStrFun) when is_binary(B) ->
  case IsStrFun(PropName) of</p>

<pre><code>true -&gt; binary_to_list(B);
_ -&gt; B
</code></pre>

<p>  end;
from_value(<em>, V, </em>) ->
  V.
{% endcodeblock %}</p>

<p>We are now able to read and write data to and from JSON format. Now we need to use the Riak client to push that into our Riak cluster.</p>

<h3>Setting up the Riak client</h3>

<p>Basho have done a great job of creating a protocol buffer-based client for use with Riak. The interface is really simple to use. Despite that, we shall create a module which will deal with this for us. This gives us a single point of abstraction of Riak and a place where we can add extra support for our own needs without spreading Riak-specific code all over the source base.</p>

<p>The first problem we need to resolve is: <em>what do we do with configuration?</em></p>

<p>This was a question I initially didn't know how to answer. After a bit of deliberation and a chat with a <a href="http://twitter.com/mononcqc" title="Ferd T-H on Twitter">respected Erlang sifu</a> (who has a <a href="http://learnyousomeerlang.com/" title="Learng you some erlang">fantastic Erlang tutorial site</a>) I decided to go with a module-based option.</p>

<p>We have our Riak cluster hidden behind the <code>haproxy</code> load balancer, and hence we have a single entry-point to connect to. If this entry-point changes, it changes for all of the clients, not just a single client. Therefore, I want the ability to manage a single set of connection information, but I want the ability to update it on the fly without having to restart the <code>csd_core</code> application. This is Erlang, after all, and modifying code and configuration on-the-fly is extremely easy. We shall abuse that.</p>

<p>We create a single module, <code>csd_riak_config.erl</code>, to contain our configuration which is referenced at start-up. It looks like this:</p>

<p>{% codeblock apps/csd_core/src/csd_riak_config.erl lang:erlang %}
-module(csd_riak_config).
-export([connection_info/0]).</p>

<p>connection_info() ->
  { "127.0.0.1", 8080 }.
{% endcodeblock %}</p>

<p>Pretty simple stuff. Let's use this functionality in our <code>gen_server</code>, and carry the configuration through from initialisation to all of the calls that will be made to the Riak server. This requires two simple modifications to the <code>csd_core_server</code> module:</p>

<p>{% codeblock apps/csd_core/src/csd_core_server.erl (part) lang:erlang %}
start_link() ->
  ConnInfo = csd_riak_config:connection_info(),
  gen_server:start_link({local, ?SERVER}, ?MODULE, [ConnInfo], []).</p>

<p>% ...</p>

<p>init([ConnInfo]) ->
  {ok, ConnInfo}.
{% endcodeblock %}</p>

<p>Confiuration is now loaded and is being passed to all of our <code>gen_server</code> callbacks. Let's make use of it. <code>csd_snippet</code> is the entry point for all snippet-related information, and one of the things that we are going to want to be able to do is write a snippet to Riak. So let's create a code-path that can do that.</p>

<h4>Writing Data to Riak</h4>

<p>The first point of call for a client is the OTP interface. Let's create an API call and a call handler to support saving snippets in <code>csd_core_server</code>:</p>

<p>{% codeblock apps/csd_core/src/csd_core_server.erl (part) lang:erlang %}</p>

<p>%% This is a simple function which invokes a call via the gen_sever
%% behaviour.
save_snippet(Snippet) ->
  gen_server:call(?SERVER, {save_snippet, Snippet}, infinity).</p>

<p>%% Handle the case where a caller wants to save a snippet to Riak. We
%% create a connection to Riak and pass that into the snippet handler
%% along with the snippet that needs to be saved. We return the newly
%% saved snippet.
handle_call({save_snippet, Snippet}, _From, ConnInfo) ->
  RiakPid = csd_riak:connect(ConnInfo),
  SavedSnippet = csd_snippet:save(RiakPid, Snippet),
  {reply, SavedSnippet, ConnInfo};
{% endcodeblock %}</p>

<p>Of course, we will need to export the <code>save_snippet()</code> function if we want to be able to call it.</p>

<p>You'll notice that we're getting the connection information passed in as the state for the OTP call, and that we're using that to create a connection to Riak via the <code>csd_riak</code> module. We shall cover this module in just a minute, but hopefully the interface to this function should make it relatively self-explanatory.</p>

<p>You might be wondering "Why are you creating the Riak client connection here instead of letting the <code>csd_snippet:save()</code> function do it by itself. It's a good question. The reason I decided to create the connection as part of OTP call rather than in the data/helper modules is because down the track there will probably be a need to do multiple interactions with Riak in a single call. If we force each of the called modules, such as <code>csd_snippet</code>, to establish their own connections then we'd probably have <em>multiple connections to Riak being created during a single client request</em>. This isn't what I would like to see happen, so it made sense (in my view) to create the client connection once and reuse it across all modules that are invoked during the request.</p>

<p>With that out of the way, we need to implement the <code>save()</code> function in the <code>csd_snippet</code> module. Brace yourself:</p>

<p><span class="filename"></span>
{% codeblock apps/csd_core/src/csd_snippet.erl (part) lang:erlang %}
save(RiakPid, Snippet={snippet, SnippetData}) ->
  case proplists:get_value(key, SnippetData, undefined) of</p>

<pre><code>undefined -&gt;
  Key = csd_riak:new_key(),
  NewSnippetData = [{key, Key} | SnippetData],
  RiakObj = csd_riak:create(?BUCKET, Key, to_json_internal(NewSnippetData)),
  ok = csd_riak:save(RiakPid, RiakObj),
  {snippet, NewSnippetData};
ExistingKey -&gt;
  RiakObj = csd_riak:fetch(RiakPid, ?BUCKET, ExistingKey),
  NewRiakObj = csd_riak:update(RiakObj, to_json_internal(SnippetData)),
  ok = csd_riak:save(RiakPid, NewRiakObj),
  Snippet
</code></pre>

<p>  end.
{% endcodeblock %}</p>

<p>On the surface this looks a little complicated, but it's actually very simple. As mentioned earlier in the post, we use a <code>key</code> property to store the identifier of the object in Riak. This code supports this notion. It works as follows:</p>

<ol>
<li><strong>Try to get the value of the <code>key</code> from the given list of properties.</strong></li>
<li><strong>If <em>not</em> found ...</strong>

<ol>
<li>create a new key using the <code>new_key()</code> function in the <code>csd_riak</code> module (this will be covered shortly).</li>
<li>Add the <code>key</code> to the list of properties for the snippet.</li>
<li>Create a new instance of a Riak object (more on this later) which contains the details of the snippet data to be written, along with the target bucket name and the key of the snippet.</li>
<li>Save the Riak object to the Riak cluster using the specified Riak client connection (Pid), and for now assume that it succeeds.</li>
<li>Return the new set of snippet data with the snippet's key included.</li>
</ol>
</li>
<li><strong>If found ...</strong>

<ol>
<li>Load the existing data from the Riak cluster into a Riak object.</li>
<li>Update the Riak object with the new data values passed into the function.</li>
<li>Save the Riak object <em>back</em> to the Riak cluster using the specified Riak client connection (Pid), and for now assume that it succeeds.</li>
<li>Return the snippet back to the caller as is.</li>
</ol>
</li>
</ol>


<p>It's fairly basic functionality which does enough to cater for our needs at this point. Through this one function, we can write new snippet instances to Riak, and we can update them too.</p>

<p>You'll also notice that another function is being called that hasn't been discussed: <code>to_snippet_internal()</code>. Rather than try to explain this, let's see the code as it's quite easy to follow:</p>

<p>{% codeblock apps/csd_core/src/csd_snippet.erl (part) lang:erlang %}
%% exported functions
to_json({snippet, SnippetData}) ->
  to_json_internal(SnippetData).</p>

<p>from_json(SnippetJson) ->
  from_json_internal(SnippetJson).</p>

<p>%% helper functions used internally.
to_json_internal(SnippetData) ->
  csd_json:to_json(SnippetData, fun is_string/1).</p>

<p>from_json_internal(SnippetJson) ->
  {snippet, csd_json:from_json(SnippetJson, fun is_string/1)}.</p>

<p>is_string(title) -> true;
is_string(left) -> true;
is_string(right) -> true;
is_string(_) -> false.
{% endcodeblock %}</p>

<p>As you can see, these are helper functions which call the <code>csd_json</code> functions to serialise/deserialise to/from JSON format. The <code>is_string()</code> function is the one that is used to tell the JSON functionality which properties are strings and which are not. At the moment, all properties defined on the snippet are string properties. Bear in mind that the <code>key</code> property, which is added automatically, is <em>not</em> a string.</p>

<p>All that is left is to see how <code>csd_riak</code> deals with the underlying Riak connectivity. Prepare to be underwhelmed!</p>

<p>{% codeblock apps/csd_core/src/csd_riak.erl lang:erlang %}
%% @spec connect(connection_info()) -> pid()
%% @doc Create a connection to the specified Riak cluster and
%%      return the Pid associated with the new connection.
connect({IP, Port}) ->
  {ok, RiakPid} = riakc_pb_socket:start_link(IP, Port),
  RiakPid.</p>

<p>%% @spec create(binary, binary, json) -> riakc_obj()
%% @doc Create a new instance of a riak object using the
%%      parameters given. The riak object can then be
%%      persisted to a Riak node/cluster. This overload
%%      assumes that the data passed in is JSON and sets
%%      the MIME type to "application/json" for you.
create(Bucket, Key, JsonData) ->
  create(Bucket, Key, JsonData, "application/json").</p>

<p>%% @spec create(binary, binary, term(), string) -> riakc_obj()
%% @doc Create a new instance of a riak object using the
%%      parameters given. The riak object can then be
%%      persisted to a Riak node/cluster. This overload
%%      takes arbitrary data and requires the user to
%%      specify the mime type of the data that is being
%%      stored.
create(Bucket, Key, Item, MimeType) ->
  RiakObj = riakc_obj:new(Bucket, Key, Item, MimeType),
  RiakObj.</p>

<p>%% @spec fetch(pid(), binary, binary) -> riakc_obj()
%% @doc Fetches a riakc object from a Riak node/cluster
%%      using the connection given.
fetch(RiakPid, Bucket, Key) ->
  RiakObj = riakc_pb_socket:get(RiakPid, Bucket, Key),
  RiakObj.</p>

<p>%% @spec update(riakc_obj(), term()) -> riakc_obj()
%% @doc Updates the stored value for a riakc object with
%%      the new one specified.
update(RiakObj, NewValue) ->
  NewRiakObj = riakc_obj:update_value(RiakObj, NewValue),
  NewRiakObj.</p>

<p>%% @spec get_value(riakc_obj()) -> term()
%% @doc Retrieves the stored value from within the riakc
%%      object.
get_value(RiakObj) ->
  Value = riakc_obj:get_value(RiakObj),
  Value.</p>

<p>%% @spec save(pid(), riakc_obj()) -> {ok, riakc_obj()} | {error | Reason}
%% @doc Saves the given riak object to the specified Riak node/cluster.
save(RiakPid, RiakObj) ->
  Result = riakc_pb_socket:put(RiakPid, RiakObj),
  Result.</p>

<p>%% @spec new_key() -> key()
%% @doc Generate an close-to-unique key that can be used to identify
%%      an object in riak. This implementation is blatantly borrowed
%%      (purloined) from the wriaki source (thanks basho!)
new_key() ->
  { {Yr, Mo, Dy}, {Hr, Mn, Sc} } = erlang:universaltime(),
  {<em>, </em>, Now} = now(),
  new_key([Yr, Mo, Dy, Hr, Mn, Sc, node(), Now]).</p>

<p>%% @spec new_key(list()) -> key()
%% @doc Generate an close-to-unique key that can be used to identify
%%      an object in riak using the given list parameter as the stuff
%%      to hash.
new_key(List) ->
  Hash = erlang:phash2(List),
  base64:encode(&lt;&lt;Hash:32>>).
{% endcodeblock %}</p>

<p>Hopefully the code in this module is fairly self-explanatory. It's a very simple API to follow which made it very easy to build. So with this in place, let's fire up the application, create a new snippet and see if it lands in the Riak store:</p>

<pre><code>oj@spawn-link  ~/blog/csd $ make webstart

   ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::22:54:55 ===
         application: csd_web
          started_at: nonode@nohost

1&gt; Snippet = csd_snippet:to_snippet(
1&gt; "Super composition!",
1&gt; "(.^) = (.) . (.)",
1&gt; "(.^) = fmap `fmap` fmap").
{snippet,[{title,"Super composition!"},
          {left,"(.^) = (.) . (.)"},
          {right,"(.^) = fmap `fmap` fmap"}]}
2&gt; SavedSnippet = csd_core_server:save_snippet(Snippet).

PROGRESS REPORT==== 4-Apr-2011::22:57:13 ===
          supervisor: {local,inet_gethost_native_sup}
             started: [{pid,&lt;0.103.0&gt;},{mfa,{inet_gethost_native,init,[[]]}}]

=PROGRESS REPORT==== 4-Apr-2011::22:57:13 ===
          supervisor: {local,kernel_safe_sup}
             started: [{pid,&lt;0.102.0&gt;},
                       {name,inet_gethost_native_sup},
                       {mfargs,{inet_gethost_native,start_link,[]}},
                       {restart_type,temporary},
                       {shutdown,1000},
                       {child_type,worker}]
{snippet,[{key,&lt;&lt;"B41kUQ=="&gt;&gt;},
          {title,"Super composition!"},
          {left,"(.^) = (.) . (.)"},
          {right,"(.^) = fmap `fmap` fmap"}]}
</code></pre>

<p>As you can see from the above script dump, a new <code>key</code> was generated for us and stored alongside the snippet (it's highlighted in bold). Verifying that the data has persisted is simple. We can hit any of the Riak nodes via its web interface. Let's take a look at <strong>http://localhost:8091/riak/snippet/B41kUQ==</strong> (your URL will have a different key):</p>

<p><img src="http://buffered.io/uploads/2010/10/localhost-verify-write.png" /></p>

<p>Great stuff! For more detail, let's see what cURL has to say:</p>

<pre><code>oj@spawn-link ~/blog/csd/ $ curl http://localhost:8091/riak/snippet/B41kUQ== -v
* About to connect() to localhost port 8091 (#0)
*   Trying ::1... Connection refused
*   Trying 127.0.0.1... connected
* Connected to localhost (127.0.0.1) port 8091 (#0)
&gt; GET /riak/snippet/B41kUQ== HTTP/1.1
&gt; User-Agent: curl/7.21.0 (x86_64-pc-linux-gnu) libcurl/7.21.0 OpenSSL/0.9.8o zlib/1.2.3.4 libidn/1.18
&gt; Host: localhost:8091
&gt; Accept: */*
&gt; 
&lt; HTTP/1.1 200 OK
&lt; X-Riak-Vclock: a85hYGBgzGDKBVIsjOy7jmcwJTLmsTJ8tuc7zpcFAA==
&lt; Vary: Accept-Encoding
&lt; Server: MochiWeb/1.1 WebMachine/1.7.3 (participate in the frantic)
&lt; Link: &lt;/riak/snippet&gt;; rel="up"
&lt; Last-Modified: Mon, 04 Apr 2011 13:13:23 GMT
&lt; ETag: "6fw7c5v4IPAsf4B5hMHybc"
&lt; Date: Mon, 04 Apr 2011 13:13:36 GMT
&lt; Content-Type: application/json
&lt; Content-Length: 107
&lt; 
* Connection #0 to host localhost left intact
* Closing connection #0
{"key":"B41kUQ==","title":"Super composition!","left":"(.^) = (.) . (.)","right":"(.^) = fmap `fmap` fmap"}
</code></pre>

<p>As you can see, it has not only serialised to JSON properly, but the MIME type has been set correctly as well.</p>

<p>This is all well and good, but we need our code to be able to read from Riak as well. That's up next.</p>

<h4>Reading Data from Riak</h4>

<p>We've already covered off what happens at the bottom level when reading data from Riak (see the above code snippet for more info). To enable this functionality at the top level, we simply need to create a <code>gen_server</code> call, handle it appropriately and expose a function in the <code>csd_snippet</code> module. Let's start at the top level:</p>

<p>{% codeblock - apps/csd_core/src/csd_core_server.erl (part) lang:erlang %}
%% OTP API function to get a snippet based on the key
get_snippet(SnippetKey) ->
  gen_server:call(?SERVER, {get_snippet, SnippetKey}, infinity).</p>

<p>%% handle the call and call the functionality from csd_snippet
handle_call({get_snippet, SnippetKey}, _From, ConnInfo) ->
  RiakPid = csd_riak:connect(ConnInfo),
  Snippet = csd_snippet:fetch(RiakPid, SnippetKey),
  {reply, Snippet, ConnInfo};
{% endcodeblock %}</p>

<p>This code is a bit of a no-brainer. It's very similar to the writing code, but just a bit simpler. Let's see what the <code>csd_snippet:fetch()</code> function looks like:</p>

<p>{% codeblock apps/csd_core/src/csd_snippet.erl (part) lang:erlang %}
fetch(RiakPid, Key) ->
  {ok, RiakObj} = csd_riak:fetch(RiakPid, ?BUCKET, Key),
  SnippetJson = csd_riak:get_value(RiakObj),
  from_json_internal(SnippetJson).
{% endcodeblock %}</p>

<p>This code just pulls a Riak object out of the back-end, extracts is value and deserialises it from JSON to our Erlang <code>proplist</code>. Very simple stuff.</p>

<p>We should be able to build this and, via the Erlang console, verify that it functions:</p>

<pre><code>3&gt; Reloading csd_core_server ... ok.
3&gt; csd_core_server:get_snippet(&lt;&lt;"B41kUQ=="&gt;&gt;).
{snippet,[{key,&lt;&lt;"B41kUQ=="&gt;&gt;},
          {title,"Super composition!"},
          {left,"(.^) = (.) . (.)"},
          {right,"(.^) = fmap `fmap` fmap"}]}
</code></pre>

<p>Works like a charm. Now, for the icing on the cake, let's get this rendering in a very simple template in our browser.</p>

<h3>End to End</h3>

<p>In order to gain access to our data in Riak from the web we need to create a new resource. This resource will respond to any URI of the form <code>/snippet/&lt;key&gt;</code>. We shall call this resource <code>csd_web_snippet_resource</code> and we'll be putting this in our web application. It looks like this:</p>

<p>{% codeblock apps/csd_web/src/csd_web_snippet_resource.erl lang:erlang %}
%% @author OJ Reeves <a href="&#109;&#97;&#x69;&#x6c;&#116;&#x6f;&#58;&#x6f;&#x6a;&#x40;&#98;&#x75;&#x66;&#102;&#x65;&#114;&#101;&#100;&#x2e;&#105;&#111;">&#x6f;&#x6a;&#x40;&#98;&#x75;&#102;&#x66;&#101;&#114;&#x65;&#100;&#46;&#105;&#111;</a>
%% @copyright 2010 OJ Reeves
%% @doc Webmachine resource that handles snippet-related actions</p>

<p>-module(csd_web_snippet_resource).
-author('OJ Reeves <a href="&#x6d;&#x61;&#105;&#108;&#116;&#x6f;&#58;&#111;&#106;&#64;&#x62;&#117;&#x66;&#102;&#x65;&#x72;&#101;&#100;&#x2e;&#x69;&#x6f;">&#111;&#x6a;&#64;&#x62;&#x75;&#x66;&#x66;&#x65;&#x72;&#x65;&#100;&#46;&#x69;&#111;</a>').</p>

<p>-export([init/1, to_html/2]).</p>

<p>-include_lib("webmachine/include/webmachine.hrl").</p>

<p>init([]) -> {ok, undefined}.</p>

<p>to_html(ReqData, State) ->
  PathInfo = wrq:path_info(ReqData),
  {ok, SnippetKey} = dict:find(key, PathInfo),
  {snippet, SnippetData} = csd_core_server:get_snippet(list_to_binary(SnippetKey)),
  {ok, Content} = snippet_dtl:render(SnippetData),
  {Content, ReqData, State}.
{% endcodeblock %}</p>

<p>As you can see, this code calls through to the <code>csd_core_server</code> to extract the data from the back-end. The value that is used as a key for the snippet is one that is pulled from the URI via Webmachine's <code>wrq:path_info()</code> function. This function extracts values from the URI based on the rules in <code>dispatch.conf</code> and provides a <a href="http://www.erlang.org/doc/man/dict.html" title="Erlang dict">dict</a> which can be used to lookup the values.</p>

<p>The code also uses a new ErlyDTL template called <code>snippet</code>. We'd best add that to the <code>templates</code> folder:</p>

<p>{% codeblock apps/csd_web/templates/snippet.dtl lang:html %}</p>

<!-- TODO : get the templating engine to stop ripping out the inline template code -->


<p><html>
  <body></p>

<pre><code>&lt;h1&gt;Snippet View&lt;/h1&gt;
&lt;h2&gt;{{ "{{ title "}} }}&lt;/h2&gt;
&lt;p&gt;Left: {{ "{{ left "}} }}&lt;/p&gt;
&lt;p&gt;Right: {{ "{{ right "}} }}&lt;/p&gt;
</code></pre>

<p>  </body>
</html>
{% endcodeblock %}</p>

<p>Finally, we just need to adjust <code>dispatch.conf</code> to include the new route handler so that our code gets called:</p>

<p>{% codeblock apps/csd_web/priv/dispatch.conf lang:erlang %}
%%-<em>- mode: erlang -</em>-
{[], csd_web_resource, []}.
{["snippet", key], csd_web_snippet_resource, []}.
{% endcodeblock %}</p>

<p>Note how <code>key</code> is specified alongside <code>"snippet"</code>. This means that the path following <code>snippet/</code> in the URI will be associated with the <code>key</code> atom in the <code>dict</code> generated by <code>wrq:path_info()</code>.</p>

<p>We're ready to rock. Rebuild, then hit the right URL, <strong>http://localhost/snippet/B41kUQ==</strong> (again, your key will be different), and you should get the following:</p>

<p><img src="http://buffered.io/uploads/2010/10/webmachine-to-riak.png" /></p>

<h2>Wrapping up</h2>

<p>Thanks for sticking with me! As you can see there is a little bit of ground-work required if you're interested in producing some form of structure that you can reuse all over your application, but the effort is definitely worth it. Now we have something in place which we can use to store arbitrarily complex <code>proplists</code> into Riak in JSON format, we have the ability to talk to Riak (read and write), and we have a proper application structure in place which we can build on.</p>

<p>Please note that the mechanism implemented in this post is quite simple and doesn't cover all cases that will be required before the application is complete. In future posts, this implementation will change to support more of those cases, such as dealing with concurrent updates, handling versions, etc.</p>

<p>Many thanks to those people who took the time out of their busy schedules to review my post before I shared it with the world. Those people shall remain nameless to protect them from any mistakes made in this post (which are solely my own).</p>

<p>As always, comments and feedback is welcomed and greatly appreciated. As are suggestions on improvements, pitfalls and blatant mistakes :)</p>

<p><strong>Note:</strong> The code for Part 3 (this post) can be found on <a href="https://bitbucket.org/OJ/csd/src/55fec468488c" title="Source code for Part 3">Github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Webmachine, ErlyDTL and Riak - Part 2]]></title>
    <link href="http://buffered.io/posts/webmachine-erlydtl-and-riak-part-2/"/>
    <updated>2010-09-12T22:15:00+10:00</updated>
    <id>http://buffered.io/posts/webmachine-erlydtl-and-riak-part-2</id>
    <content type="html"><![CDATA[<p><img src="http://buffered.io/uploads/2010/09/riak-logo.png" alt="Riak Logo" style="float:left;padding-right:5px;padding-bottom:5px;"/>In <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a> of the series we covered the basics of getting the development environment up and running. We also looked at how to get a really simple ErlyDTL template rendering. If you haven't yet gone through Part 1, I suggest you do that now. If you have, read on!</p>

<p>There are a few reasons this series is targeting this technology stack. One of them is <strong>uptime</strong>. We're aiming to build a site that stays up as much as possible. Given that, one of the things that I missed in the previous post was setting up a <a href="http://en.wikipedia.org/wiki/Load_balancing_(computing)" title="Load balancing">load balancer</a>. Hence this post will attempt to fill that gap.</p>

<!--more-->


<p>Any serious web-based application will have load-balancing in play somewhere. While not essential during development, it's handy to have a similar set up in the hope that it exposes you to some potential issues you might face when the application reaches production.</p>

<p>There are many high-quality load-balancing solutions out there to choose from. For this series, we shall be using <a href="http://haproxy.1wt.eu/" title="HAProxy">HAProxy</a>, which is a common choice amongst developers building scalable web applications. The rest of this post will cover how to set up HAProxy, verifying that the configuration is correct and confirming that it behaves appropriately when nodes in our cluster go down.</p>

<p>Please note the goal is to demonstrate how HAProxy <em>can</em> be configured. When deploying to your production environments please make sure the configuration matches your needs.</p>

<h3>HAProxy installation</h3>

<p>Let's start by pulling down the latest stable version of HAProxy's source, extracting it and building it. Here's a sample log of what you should expect:</p>

<pre><code>oj@nix ~/blog $ wget http://haproxy.1wt.eu/download/1.4/src/haproxy-1.4.14.tar.gz

  ... snip ...

oj@nix ~/blog $ tar -xzf haproxy-1.4.14.tar.gz 

  ... snip ...
</code></pre>

<p>At this point we've got the source and we're ready to make. HAProxy requires a parameter in order to build, and this parameter varies depending on your target system:</p>

<pre><code>oj@nix ~/blog $ cd haproxy-1.4.14
oj@nix ~/blog/haproxy-1.4.14 $ make

Due to too many reports of suboptimized setups, building without
specifying the target is no longer supported. Please specify the
target OS in the TARGET variable, in the following form:

   make TARGET=xxx

Please choose the target among the following supported list :

   linux26, linux24, linux24e, linux22, solaris
   freebsd, openbsd, cygwin, custom, generic

Use "generic" if you don't want any optimization, "custom" if you
want to precisely tweak every option, or choose the target which
matches your OS the most in order to gain the maximum performance
out of it. Please check the Makefile in case of doubts.
make: *** [all] Error 1
</code></pre>

<p>According to <a href="http://en.wikipedia.org/wiki/Uname" title="uname">uname</a>, I'm running Linux Kernel 2.6:</p>

<pre><code>oj@nix ~/blog/haproxy-1.4.14 $ uname -r
2.6.31-21-generic
</code></pre>

<p>As a result, I'll be passing in <strong>linux26</strong>. Make sure you specify the correct option depending on which system you are running. We'll be building it <em>and</em> installing it so that it can be called from anywhere:</p>

<pre><code>oj@nix ~/blog/haproxy-1.4.14 $ make TARGET=linux26

    ... snip ...

oj@nix ~/blog/haproxy-1.4.14 $ sudo make install

   ... snip ...
</code></pre>

<p>Simple! We now need to create a configuration for HAProxy which we can use during development. Not surprisingly, HAProxy can be run as a daemon, but it can also be invoked from the command line with a configuration passed as a parameter. For our development, we'll be executing from the command line as this will give us feedback/output on what's going on.</p>

<p>Let's create a file called <code>dev.haproxy.conf</code> inside our application directory so that it can be included in our source:</p>

<p>{% codeblock dev.haproxy.conf lang:bash %}</p>

<h1>start with the global settings which will</h1>

<h1>apply to all sections in the configuration.</h1>

<p>global
  # specify the maximum connections across the board
  maxconn 2048
  # enable debug output
  debug</p>

<h1>now set the default settings for each sub-section</h1>

<p>defaults
  # stick with http traffic
  mode http
  # set the number of times HAProxy should attempt to
  # connect to the target
  retries 3
  # specify the number of connections per front and
  # back end
  maxconn 1024
  # specify some timeouts (all in milliseconds)
  timeout connect 5000
  timeout client 50000
  timeout server 50000</p>

<h6>##### Webmachine Configuration</h6>

<h1>here is the first of the front-end sections.</h1>

<h1>this is where we specify our webmachine instances.</h1>

<h1>in our case we start with just one instance, but</h1>

<h1>we can add more later</h1>

<p>frontend webfarm
  # listen on port 80 across all network interfaces
  bind *:80
  # by default, point at our backend configuration
  # which lists our webmachine instances (this is
  # configured below in another section)
  default_backend webmachines</p>

<h1>this section indicates how the connectivity to</h1>

<h1>all the instances of webmachine should work.</h1>

<h1>Again, for dev there is only one instance, but</h1>

<h1>in production there would be more.</h1>

<p>backend webmachines
  # we'll specify a round-robin configuration in
  # case we add nodes down the track.
  balance roundrobin
  # enable the "X-Forware-For" header so that
  # we can see the client's IP in Webmachine,
  # not just the proxy's address
  option forwardfor
  # later down the track we'll be making the use
  # of cookies for various reasons. So we'll
  # enable support for this while we're here.
  cookie SERVERID insert nocache indirect
  # list the servers who are to be balanced
  # (just the one in the case of dev)
  server Webmachine1 127.0.0.1:8000</p>

<h6>##### Riak Configuration</h6>

<h1>We are yet to touch Riak so far, but given that</h1>

<h1>this post is going to cover the basics of</h1>

<h1>connectivity, we'll cover off the configuration</h1>

<h1>now so we don't have to do it later.</h1>

<p>frontend dbcluster
  # We'll be using protocol buffers to talk to
  # Riak, so we will change from the default mode
  # and use tcp instead
  mode tcp
  # we're only interested in allowing connections
  # from internal sources (so that we don't expose
  # ourselves to the web. so we shall only listen
  # on an internal interface on port 8080
  bind 127.0.0.1:8080
  # Default to the riak cluster configuration
  default_backend riaks</p>

<h1>Here is the magic bit which load balances across</h1>

<h1>our three instances of riak which are clustered</h1>

<h1>together</h1>

<p>backend riaks
  # again, make sure we specify tcp instead of
  # the default http mode
  mode tcp
  # use a standard round robin approach for load
  # balancing
  balance roundrobin
  # list the three servers as optional targets
  # for load balancing - these are what we set
  # up during Part 1. Add health-checking as
  # well so that when nodes go down, HAProxy
  # can remove them from the cluster
  server Riak1 127.0.0.1:8081 check
  server Riak2 127.0.0.1:8082 check
  server Riak3 127.0.0.1:8083 check
{% endcodeblock %}</p>

<p>In the configuration above the <code>backend riaks</code> section has three server nodes. Each one of them has the <code>check</code> option specified. This enables health-checking on the same address and port that the server instance is bound to. If you decided that you didn't want to do health-checking in this manner you easily enable health-checking over HTTP, as Riak has a built-in URI which can be used to validate the state of the node. Change the <code>backend riaks</code> section in the configuration to look like this:
{% codeblock lang:bash %}</p>

<h1>Here is the magic bit which load balances across</h1>

<h1>our three instances of riak which are clustered</h1>

<h1>together</h1>

<p>backend riaks
  # again, make sure we specify tcp instead of
  # the default http mode
  mode tcp
  # use a standard round robin approach for load
  # balancing
  balance roundrobin
  # enable HTTP health checking using the GET method
  # on the URI "/ping". This URI is part of Riak and
  # can be used to determine if the node is up.
  # We specify that we want to use the GET action, and
  # use the URI "/ping" - this is the RESTful health
  # check URI that comes as part of Riak.
  option httpchk GET /ping
  # list the three servers as optional targets
  # for load balancing - these are what we set
  # up during Part 1. Add health-checking as
  # well so that when nodes go down, HAProxy
  # can remove them from the cluster.</p>

<p>  # change the health-check address of the node to 127.0.0.0:8091
  # which is the REST interface for the first Riak node
  server Riak1 127.0.0.1:8081 check addr 127.0.0.1 port 8091</p>

<p>  # change the health-check address of the node to 127.0.0.0:8092
  # which is the REST interface for the second Riak node
  server Riak2 127.0.0.1:8082 check addr 127.0.0.1 port 8092</p>

<p>  # change the health-check address of the node to 127.0.0.0:8093
  # which is the REST interface for the third Riak node
  server Riak3 127.0.0.1:8083 check addr 127.0.0.1 port 8093
{% endcodeblock %}</p>

<p>To make sure this is functioning correctly, we need to open two consoles and change our working directory to our <code>csd</code> application (for those who have forgotten, <code>csd</code> is the application we're building - it was mentioned in <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a>). In console 1:</p>

<pre><code>oj@nix ~/blog/csd $ sudo haproxy -f dev.haproxy.conf -d
Available polling systems :
     sepoll : pref=400,  test result OK
      epoll : pref=300,  test result OK
       poll : pref=200,  test result OK
     select : pref=150,  test result OK
Total: 4 (4 usable), will use sepoll.
Using sepoll() as the polling mechanism.
</code></pre>

<p>This indicates that HAProxy is up and running and waiting for connections. Let's get Webmachine fired up in console 2:</p>

<pre><code>oj@nix ~/blog/csd $ ./start.sh

    ... snip ...

=PROGRESS REPORT==== 4-Apr-2011::23:39:27 ===
         application: csd
          started_at: nonode@nohost
</code></pre>

<p>Now Webmachine is fired up with our application running. We should be able to hit our page, this time at <a href="http://localhost/" title="localhost">localhost</a>, and see exactly what we saw at the end of <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a>.</p>

<p><img src="http://buffered.io/uploads/2010/09/haproxy-validation.png" /></p>

<h3>Verification of HAProxy configuration</h3>

<p>On the surface it appears that we haven't broken anything. We also need to make sure that any communication with Riak that we have down the track is also functioning. So let's validate that now.</p>

<p>First, we have to make sure that Riak is running. If you have followed <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a> already and your Riak cluster is running then you're good to go. If not, please read <a href="/posts/webmachine-erlydtl-and-riak-part-1/" title="Wembachine, ErlyDTL and Riak - Part 1">Part 1</a> for information on how to install Riak and configure it to run as a cluster of 3 nodes.</p>

<p>Next, let's create 3 new connections and use the <a href="https://github.com/basho/riak-erlang-client/blob/master/src/riakc_pb_socket.erl#L181" title="riakc_pb_socket:get_server_info/1">get_server_info/1</a> function to find out which node we are connected to. To do this, we'll need to use an Erlang console which has all the Riak dependencies ready to go. It just so happens that when we fired up our Webmachine instance, we got an Erlang console for free. Simply hit the <code>enter</code> key and you'll be given a prompt. Notice that when we connect to Riak using the <a href="https://github.com/basho/riak-erlang-client/blob/master/src/riakc_pb_socket.erl#L97" title="riakc_pb_socket:start_link/2">start_link/2</a> function, we are passing in the IP address and port of the load-balanced cluster instead of one of the running Riak nodes:
{% codeblock lang:erlang %}
1> {ok, C1} = riakc_pb_socket:start_link("127.0.0.1", 8080).</p>

<p>=PROGRESS REPORT==== 4-Apr-2011::23:41:18 ===</p>

<pre><code>      supervisor: {local,inet_gethost_native_sup}
         started: [{pid,&lt;0.148.0&gt;},{mfa,{inet_gethost_native,init,[[]]}}]
</code></pre>

<p>=PROGRESS REPORT==== 4-Apr-2011::23:41:18 ===</p>

<pre><code>      supervisor: {local,kernel_safe_sup}
         started: [{pid,&lt;0.147.0&gt;},
                   {name,inet_gethost_native_sup},
                   {mfargs,{inet_gethost_native,start_link,[]}},
                   {restart_type,temporary},
                   {shutdown,1000},
                   {child_type,worker}]
</code></pre>

<p>{ok,&lt;0.146.0>}
2> riakc_pb_socket:get_server_info(C1).
{ok,[{node,&lt;&lt;"dev1@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>3> {ok, C2} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.151.0>}
4> riakc_pb_socket:get_server_info(C2).
{ok,[{node,&lt;&lt;"dev2@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>5> {ok, C3} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.154.0>}
6> riakc_pb_socket:get_server_info(C3).
{ok,[{node,&lt;&lt;"dev3@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>{% endcodeblock %}</p>

<p>So we can see that the load balancer has allocated three different connections, each to a different node in the cluster. This is a good sign. So let's kill off one of the nodes:</p>

<pre><code>oj@nix ~/blog/riak/dev $ dev2/bin/riak stop
ok
</code></pre>

<p>In a very short period of time, you should see output in the HAProxy console which looks something like this:</p>

<pre><code>[WARNING] 253/235636 (11824) : Server riaks/Riak2 is DOWN, reason: Layer4 connection problem, info: "Connection refused", check duration: 0ms.
</code></pre>

<p>The load balancer noticed that the node has died. Let's make sure it no longer attempts to allocate connections to <code>dev2</code>. Note that we call <a href="http://www.erlang.org/documentation/doc-5.2/doc/getting_started/getting_started.html" title="Getting started">f()</a> in our console before running the same script again, as this forces the shell to forget about any existing variable bindings:
{% codeblock lang:erlang %}
7> f().
ok
8> {ok, C1} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.1951.0>}
9> riakc_pb_socket:get_server_info(C1).
{ok,[{node,&lt;&lt;"dev1@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>10> {ok, C2} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.1954.0>}
11> riakc_pb_socket:get_server_info(C2).
{ok,[{node,&lt;&lt;"dev3@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>12> {ok, C3} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.1957.0>}
13> riakc_pb_socket:get_server_info(C3).
{ok,[{node,&lt;&lt;"dev1@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>{% endcodeblock %}</p>

<p>As we hoped, <code>dev2</code> is nowhere to be seen. Let's fire it up again:</p>

<pre><code>oj@nix ~/blog/riak/dev $ dev2/bin/riak start
</code></pre>

<p><strong>Note:</strong> It isn't necessary to tell the node to rejoin the cluster. This happens automatically. Thanks to Siculars (see comment thread) for pointing that out.</p>

<p>HAProxy's console will show you that it has re-established a connection to <code>dev2</code></p>

<pre><code>[WARNING] 253/235852 (11824) : Server riaks/Riak2 is UP, reason: Layer7 check passed, code: 200, info: "OK", check duration: 1ms.
</code></pre>

<p>As a final test, let's make sure we see that node get connections when we attempt to connect:
{% codeblock lang:erlang %}
14> f().
ok
15> {ok, C1} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.4203.0>}
16> riakc_pb_socket:get_server_info(C1).
{ok,[{node,&lt;&lt;"dev3@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>17> {ok, C2} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.4206.0>}
18> riakc_pb_socket:get_server_info(C2).
{ok,[{node,&lt;&lt;"dev1@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>19> {ok, C3} = riakc_pb_socket:start_link("127.0.0.1", 8080).
{ok,&lt;0.4209.0>}
20> riakc_pb_socket:get_server_info(C3).
{ok,[{node,&lt;&lt;"dev2@127.0.0.1">>},</p>

<pre><code> {server_version,&lt;&lt;"0.12.0"&gt;&gt;}]}
</code></pre>

<p>{% endcodeblock %}</p>

<h3>Wrapping up</h3>

<p>Excellent. Now that we've got our load-balancer set up in development, we're ready to dive into connecting to Riak from our <code>csd</code> application. That will be the topic for the next post in this series.</p>

<p>As always, comments and feedback are welcome and greatly appreciated. Suggestions on improvements and pointers on mistakes would be awesome. To anyone out there who has put HAProxy into production, we would love to hear your comments on your configuration!</p>

<p><strong>Note:</strong> The code for Part 2 (this post) can be found on <a href="https://github.com/OJ/csd/tree/Part2-20110403" title="Source Code for Part 2">Github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Webmachine, ErlyDTL and Riak - Part 1]]></title>
    <link href="http://buffered.io/posts/webmachine-erlydtl-and-riak-part-1/"/>
    <updated>2010-09-01T23:29:00+10:00</updated>
    <id>http://buffered.io/posts/webmachine-erlydtl-and-riak-part-1</id>
    <content type="html"><![CDATA[<p><img src="http://buffered.io/uploads/2010/09/riak-logo.png" alt="Riak Logo" style="float:left;padding-right:5px;padding-bottom:5px;"/>It has been a long time coming, but the first post is finally here! This is the first in a series of post, as <a href="/posts/the-future-is-erlang/" title="The Future is Erlang">promised a while ago</a>, covering off web development using <a href="http://erlang.org/" title="Erlang">Erlang</a>. This post is the ubiquitous "get up and running" post, which aims to get your environment set up so that you can dive in to development. The next post will detail how to handle a basic end-to-end web request.</p>

<p><img src="http://buffered.io/uploads/2010/09/Erlang_logo.png" width="150" style="float:right;margin-left:5px;margin-bottom:5px;"/>First up, a few things we need to be aware of before we begin:</p>

<ol>
<li>The information in this post has only been verified on Linux (<a href="http://linuxmint.com/" title="Linux Mint">Mint</a> to be exact). It <em>should</em> work just fine on Mac OSX. I'm almost certain that it <em>won't</em> work on a Windows machine. So if you're a Windows developer, you'll have to wait for another post down the track which covers off how to get your environment ready to rock.</li>
<li>We'll be downloading, building and installing <a href="http://erlang.org/" title="Erlang">Erlang</a>, <a href="http://github.com/evanmiller/erlydtl" title="ErlyDTL">ErlyDTL</a>, <a href="http://www.basho.com/developers.html#Riak" title="Riak">Riak</a> and <a href="http://www.basho.com/developers.html#Webmachine" title="Webmachine">Webmachine</a>.</li>
<li><a href="http://www.basho.com/developers.html#Rebar" title="Rebar">Rebar</a> is the tool we'll be using to handle builds, but I won't be covering it in any depth.</li>
<li>You will need the latest versions of both <a href="http://hg-scm.com/" title="Mercurial">Mercurial</a> and <a href="http://git-scm.com/" title="Git">Git</a> so make sure they're downloaded and installed before you follow this article.</li>
<li>We'll be doing <em>some</em> interaction with Riak via <a href="http://curl.haxx.se/" title="cURL and libcurl">curl</a>, so make sure you have it downloaded and installed as well.</li>
<li>This is intended to be a step-by-step guide targeted at those who are very new to web development in Erlang. This may not be the most ideal set up, nor the best way of doing certain things. I am hoping that those people who are more experienced than I will be able to provide feedback and guidance in areas where I am lacking.</li>
<li>Over the course of this series I'll be attempting to build an Erlang version of the <a href="http://bitbucket.org/OJ/codesmackdown" title="Code Smackdown">Code Smackdown</a> site that I've been working on here and there with a <a href="http://secretgeek.net/" title="secretGeek">mate of mine</a>. You'll see that the sample application we're working on is called "csd" for obvious reasons.</li>
</ol>


<p>OK, let's get into it. First up, Erlang.</p>

<!--more-->


<h3>Installing Erlang R14B02</h3>

<p>Download and installation is fairly simple. Right now we're not worried about enabling all of the features of Erlang, such as interfacing with Java and providing support for GTK. So the boilerplate functionality is enough. Here are the steps to follow:</p>

<pre><code>oj@nix ~/blog $ wget http://erlang.org/download/otp_src_R14B02.tar.gz

  ... snip ...

oj@nix ~/blog $ tar -xzf otp_src_R14B02.tar.gz 
oj@nix ~/blog $ cd otp_src_R14B02/
oj@nix ~/blog/otp_src_R14B02 $ ./configure 

  ... snip ...

oj@nix ~/blog/otp_src_R14B02 $ make

  ... snip ...

oj@nix ~/blog/otp_src_R14B02 $ sudo make install

  ... snip ...
</code></pre>

<p>Done! Let's confirm that it has been set up correctly:</p>

<pre><code>oj@nix ~/blog $ erl
Erlang R14B02 (erts-5.8.3) [source] [64-bit] [smp:2:2] [rq:2] [async-threads:0] [hipe] [kernel-poll:false]

Eshell V5.8.3  (abort with ^G)
1&gt; q().
ok
</code></pre>

<p>Excellent. Next let's get Riak going.</p>

<h3>Installing Riak 0.14</h3>

<p>Considering the power of the software you are about to set up, it is absolutely insane how easy it is to get it running. If any of you have tried to get <a href="http://couchdb.apache.org/" title="CouchDB">CouchDB</a> running you'll no doubt have experienced a few quirks and a bit of pain getting it rolling. Not so with Riak. As mentioned at the start of the article, make sure you have a recent version of <a href="http://hg-scm.com/" title="Mercurial">Mercurial</a> and <a href="http://git-scm.com/" title="Git">Git</a> installed.</p>

<pre><code>oj@nix ~/blog$ hg --version
Mercurial Distributed SCM (version 1.7.3)
(see http://mercurial.selenic.com for more information)

Copyright (C) 2005-2010 Matt Mackall and others
This is free software; see the source for copying conditions. There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

oj@nix ~/blog$ git --version
git version 1.7.3.5

oj@nix ~/blog $ git clone git://github.com/basho/riak
Cloning into riak...
remote: Counting objects: 10812, done.
remote: Compressing objects: 100% (3468/3468), done.
remote: Total 10812 (delta 7217), reused 10469 (delta 7020)
Receiving objects: 100% (10812/10812), 8.83 MiB | 729 KiB/s, done.
Resolving deltas: 100% (7217/7217), done.

oj@nix ~/blog $ cd riak
oj@nix ~/blog/riak $ make
./rebar get-deps
==&gt; rel (get-deps)
==&gt; riak (get-deps)
Pulling cluster_info from {git,"git://github.com/basho/cluster_info",
                               {branch,"master"}}
Cloning into cluster_info...
Pulling luwak from {git,"git://github.com/basho/luwak",{branch,"master"}}
Cloning into luwak...
Pulling riak_kv from {git,"git://github.com/basho/riak_kv",{branch,"master"}}
Cloning into riak_kv...
Pulling riak_err from {git,"git://github.com/basho/riak_err",
                           {branch,"master"}}
Cloning into riak_err...
==&gt; cluster_info (get-deps)
==&gt; riak_kv (get-deps)
Pulling riak_core from {git,"git://github.com/basho/riak_core",
                            {branch,"master"}}
Cloning into riak_core...
Pulling riakc from {git,"git://github.com/basho/riak-erlang-client",
                        {tag,"riakc-1.0.2"}}
Cloning into riakc...
Pulling luke from {git,"git://github.com/basho/luke",{tag,"luke-0.2.3"}}
Cloning into luke...
Pulling erlang_js from {git,"git://github.com/basho/erlang_js",
                            {tag,"erlang_js-0.5.0"}}
Cloning into erlang_js...
Pulling bitcask from {git,"git://github.com/basho/bitcask",{branch,"master"}}
Cloning into bitcask...
Pulling ebloom from {git,"git://github.com/basho/ebloom",{branch,"master"}}
Cloning into ebloom...
Pulling eper from {git,"git://github.com/dizzyd/eper.git",{branch,"master"}}
Cloning into eper...
==&gt; riak_core (get-deps)
Pulling protobuffs from {git,"git://github.com/basho/erlang_protobuffs",
                             {tag,"protobuffs-0.5.1"}}
Cloning into protobuffs...
Pulling basho_stats from {git,"git://github.com/basho/basho_stats","HEAD"}
Cloning into basho_stats...
Pulling riak_sysmon from {git,"git://github.com/basho/riak_sysmon",
                              {branch,"master"}}
Cloning into riak_sysmon...
Pulling webmachine from {git,"git://github.com/basho/webmachine",
                             {tag,"webmachine-1.8.0"}}
Cloning into webmachine...
==&gt; protobuffs (get-deps)
==&gt; basho_stats (get-deps)
==&gt; riak_sysmon (get-deps)
==&gt; webmachine (get-deps)
Pulling mochiweb from {git,"git://github.com/basho/mochiweb",
                           {tag,"mochiweb-1.7.1"}}
Cloning into mochiweb...
==&gt; mochiweb (get-deps)
==&gt; riakc (get-deps)
==&gt; luke (get-deps)
==&gt; erlang_js (get-deps)
==&gt; ebloom (get-deps)
==&gt; bitcask (get-deps)
==&gt; eper (get-deps)
==&gt; luwak (get-deps)
Pulling skerl from {git,"git://github.com/basho/skerl",{tag,"skerl-1.0.1"}}
Cloning into skerl...
==&gt; skerl (get-deps)
==&gt; riak_err (get-deps)
./rebar compile
==&gt; cluster_info (compile)
Compiled src/cluster_info_ex.erl


  ... snip ...
</code></pre>

<p>I snipped a lot of the make output for obvious reasons. Let's build a few development nodes of Riak and cluster them together as indicated in the <a href="https://wiki.basho.com/display/RIAK/The+Riak+Fast+Track" title="Riak Fast Track">Riak Fast Track</a>:</p>

<pre><code>oj@nix ~/blog/riak $ make devrel
mkdir -p dev
(cd rel &amp;&amp; ../rebar generate target_dir=../dev/dev1 overlay_vars=vars/dev1_vars.config)
==&gt; rel (generate)
mkdir -p dev
(cd rel &amp;&amp; ../rebar generate target_dir=../dev/dev2 overlay_vars=vars/dev2_vars.config)
==&gt; rel (generate)
mkdir -p dev
(cd rel &amp;&amp; ../rebar generate target_dir=../dev/dev3 overlay_vars=vars/dev3_vars.config)
==&gt; rel (generate)

oj@nix ~/blog/riak $ cd dev
oj@nix ~/blog/riak/dev $ dev1/bin/riak start
oj@nix ~/blog/riak/dev $ dev2/bin/riak start
oj@nix ~/blog/riak/dev $ dev3/bin/riak start
oj@nix ~/blog/riak/dev $ dev2/bin/riak-admin join dev1
Sent join request to dev1

oj@nix ~/blog/riak/dev $ dev3/bin/riak-admin join dev1
Sent join request to dev1

oj@nix ~/blog/riak/dev $ curl -H "Accept: text/plain" http://127.0.0.1:8091/stats
{
  ... snip ...

  "nodename": "dev1@127.0.0.1",
    "connected_nodes": [
    "dev2@127.0.0.1",
    "dev3@127.0.0.1"
  ],

  ... snip ...

  "ring_members": [
    "dev1@127.0.0.1",
    "dev2@127.0.0.1",
    "dev3@127.0.0.1"
  ],
  "ring_num_partitions": 64,
  "ring_ownership": "[{'dev3@127.0.0.1',21},{'dev2@127.0.0.1',21},{'dev1@127.0.0.1',22}]",

  ... snip ...
}
</code></pre>

<p>As we can see from the curl output, we now have a 3-node Riak cluster up and running. Those three nodes have the following traits:</p>

<table border="1">
  <thead>
    <tr>
      <th>Name</th>
      <th>Protobuf Port</th>
      <th>HTTP Port</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>dev1@127.0.0.1</td>
      <td>8081</td>
      <td>8091</td>
    </tr>
    <tr>
      <td>dev2@127.0.0.1</td>
      <td>8082</td>
      <td>8092</td>
    </tr>
    <tr>
      <td>dev3@127.0.0.1</td>
      <td>8083</td>
      <td>8093</td>
    </tr>
  </tbody>
</table>


<p>We can talk to any of these nodes and they will replicate their data to the other nodes. Nifty! Now that we have a Riak cluster running for development, let's get Webmachine ready.</p>

<h3>Installing Webmachine 0.8</h3>

<p>Again, the process is very simple:</p>

<pre><code>oj@nix ~/blog $ git clone git://github.com/basho/webmachine
loning into webmachine...
remote: Counting objects: 1183, done.
remote: Compressing objects: 100% (484/484), done.
remote: Total 1183 (delta 735), reused 1063 (delta 668)
Receiving objects: 100% (1183/1183), 1.17 MiB | 294 KiB/s, done.
Resolving deltas: 100% (735/735), done.

oj@nix ~/blog $ cd webmachine/
oj@nix ~/blog/webmachine $ make
==&gt; webmachine (get-deps)
Pulling mochiweb from {git,"git://github.com/mochi/mochiweb",{tag,"1.5.1"}}
Cloning into mochiweb...
==&gt; mochiweb (get-deps)
==&gt; mochiweb (compile)
Compiled src/mochiglobal.erl
Compiled src/mochiweb_sup.erl

  ... snip ...
</code></pre>

<p>As you can see, Webmachine sits on top of the <a href="http://github.com/mochi/mochiweb" title="Mochiweb">Mochiweb</a> web server.</p>

<p>To create our own application which sits on top of Webmachine, we can utilise the <code>new_webmachine.sh</code> script. So let's do that to create our Code Smackdown (csd) site:</p>

<pre><code>oj@nix ~/blog/webmachine $ scripts/new_webmachine.sh
usage: new_webmachine.sh name [destdir]
oj@nix ~/blog/webmachine $ scripts/new_webmachine.sh csd ..
==&gt; priv (create)
Writing /home/oj/blog/csd/README
Writing /home/oj/blog/csd/Makefile
Writing /home/oj/blog/csd/rebar.config
Writing /home/oj/blog/csd/rebar
Writing /home/oj/blog/csd/start.sh
Writing /home/oj/blog/csd/src/csd.app.src
Writing /home/oj/blog/csd/src/csd.erl
Writing /home/oj/blog/csd/src/csd_app.erl
Writing /home/oj/blog/csd/src/csd_sup.erl
Writing /home/oj/blog/csd/src/csd_resource.erl
Writing /home/oj/blog/csd/priv/dispatch.conf
</code></pre>

<p>Webmachine generates a fully functional website out of the box. So we should be able to build it, fire it up and see it in action:</p>

<pre><code>oj@nix ~/blog/webmachine $ cd ../csd
oj@nix ~/blog/csd $ make
==&gt; csd (get-deps)
Pulling webmachine from {git,"git://github.com/basho/webmachine","HEAD"}
Cloning into webmachine...
==&gt; webmachine (get-deps)
Pulling mochiweb from {git,"git://github.com/mochi/mochiweb",{tag,"1.5.1"}}
Cloning into mochiweb...
==&gt; mochiweb (get-deps)
==&gt; mochiweb (compile)
Compiled src/mochiglobal.erl

  ... snip ...

oj@nix ~/blog/csd $ ./start.sh
Erlang R14B02 (erts-5.8.3) [source] [64-bit] [smp:2:2] [rq:2] [async-threads:0] [hipe] [kernel-poll:false]

  ... snip ...

PROGRESS REPORT==== 3-Apr-2011::22:38:36 ===
          supervisor: {local,csd_sup}
             started: [{pid,&lt;0.76.0&gt;},
                       {name,webmachine_mochiweb},
                       {mfargs,
                           {webmachine_mochiweb,start,
                               [[{ip,"0.0.0.0"},
                                 {port,8000},
                                 {log_dir,"priv/log"},
                                 {dispatch,[{[],csd_resource,[]}]}]]}},
                       {restart_type,permanent},
                       {shutdown,5000},
                       {child_type,worker}]

=PROGRESS REPORT==== 3-Apr-2011::22:38:36 ===
         application: csd
          started_at: nonode@nohost
</code></pre>

<p>The application is now up and running. As you can see from the output, our csd application has been fired up and is listening on port 8000. Let's fire it up in a web browser to see if it works.</p>

<p><img src="http://buffered.io/uploads/2010/09/wm_default.png"/></p>

<p>It's alive! We're almost done. Before we finish up, let's get set up our build to include some dependencies.</p>

<h3>Adding ErlyDTL and Riak Client Dependencies</h3>

<p>Rebar makes this bit a walk in the park (thanks <a href="http://dizzyd.com/" title="Gradual Epiphany">Dave</a>, you rock!). Just make sure you stop your Webmachine node before continuing by typing <code>q().</code> into your Erlang console.</p>

<p>The <code>rebar.config</code> file is what drives rebar's dependency mechanism. We need to open this file and add the entries we need to include in our application. Webmachine's <code>start.sh</code> script by default includes all of the dependencies on start up, so after modifying the configuration, we don't have to do anything else (other than use the library of course).</p>

<p>Open up <code>rebar.config</code> in your <a href="http://www.vim.org/" title="VIM">favourite editor</a>, it should look something like this:</p>

<p>{% codeblock rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-</p>

<p>{deps, [{webmachine, "1.8.*", {git, "git://github.com/basho/webmachine", "HEAD"}}]}.
{% endcodeblock %}</p>

<p>Edit the file so that it includes both ErlyDTL and the Riak Client:</p>

<p>{% codeblock rebar.config lang:erlang %}
%%-<em>- mode: erlang -</em>-
{deps,
  [</p>

<pre><code>{webmachine, "1.8.*", {git, "git://github.com/basho/webmachine", "HEAD"}},
{riakc, ".*", {git, "git://github.com/basho/riak-erlang-client", "HEAD"}},
{erlydtl, "0.6.1", {git, "git://github.com/OJ/erlydtl.git", "HEAD"}}
</code></pre>

<p>  ]
}.
{% endcodeblock %}</p>

<p>You'll notice that the <code>erlydtl</code> reference points at my own fork of the ErlyDTL project. This is because I have made it compile cleanly with rebar so that any dependent projects are also able to be build with rebar. Feel free to use your own fork if you like, but mine is there if you can't be bothered :)</p>

<p>Save the file and build!</p>

<pre><code>oj@nix ~/blog/csd $ make
==&gt; mochiweb (get-deps)
==&gt; webmachine (get-deps)
==&gt; csd (get-deps)
Pulling riakc from {git,"git://github.com/basho/riak-erlang-client","HEAD"}
Cloning into riakc...
Pulling erlydtl from {git,"git://github.com/OJ/erlydtl.git","HEAD"}
Cloning into erlydtl...
==&gt; riakc (get-deps)
Pulling protobuffs from {git,"git://github.com/basho/erlang_protobuffs",
                             {tag,"protobuffs-0.5.1"}}
Cloning into protobuffs...
==&gt; protobuffs (get-deps)
==&gt; erlydtl (get-deps)
==&gt; mochiweb (compile)
==&gt; webmachine (compile)
==&gt; protobuffs (compile)
Compiled src/pokemon_pb.erl
Compiled src/protobuffs_parser.erl

  ... snip ...
</code></pre>

<p>Dependencies sorted. For the final part of this blog post, we'll include a basic ErlyDTL template and use it to render the page so we can see how it works.</p>

<h3>Rendering an ErlyDTL Template</h3>

<p>Rebar has built-in support for the compilation of ErlyDTL templates. It can be configured to behave how you want it to, but out of the box it...</p>

<ul>
<li>... looks for <code>*.dtl</code> files in the <code>./templates</code> folder</li>
<li>... compiles each of the found templates into a module called <code>filename_dtl</code> (eg. <code>base.dtl</code> becomes the module base_dtl)</li>
<li>... puts the module beam files into the <code>ebin</code> directory</li>
</ul>


<p>Very handy. Let's create a very simple template by creating a <code>templates</code> folder, and editing a new file in that folder called <code>sample.dtl</code></p>

<p>{% codeblock templates/sample.dtl lang:html %}
<html><body>Hello from inside ErlyDTL. You passed in {{ "{" }}{ param }}.</body></html>
{% endcodeblock %}</p>

<p>Then open up <code>src/csd_resource.erl</code> and search for the <code>to_html()</code> function. It should look like this:</p>

<p>{% codeblock src/csd_resource.erl lang:erlang %}
to_html(ReqData, State) -></p>

<pre><code>{"&lt;html&gt;&lt;body&gt;Hello, new world&lt;/body&gt;&lt;/html&gt;", ReqData, State}.
</code></pre>

<p>{% endcodeblock %}</p>

<p>Modify it to look like this:</p>

<p>{% codeblock src/csd_resource.erl lang:erlang %}
to_html(ReqData, State) -></p>

<pre><code>{ok, Content} = sample_dtl:render([{param, "Slartibartfast"}]),
{Content, ReqData, State}.
</code></pre>

<p>{% endcodeblock %}</p>

<p>For now, don't worry about the content of this file. I will cover this off in a future post.</p>

<p>In the past, we had to manually modify <code>ebin/csd.app</code> to include the template that we've just created. Thankfully, <code>rebar</code> has been updated so that it generates the <code>ebin/csd.app</code> file from the <code>src/csd.app.src</code> file automatically when the application is built. <code>rebar</code> adds the required modules from the <code>src</code> folder <em>and</em> includes the templates from the <code>templates</code> folder. Therefore, with our template and module ready to go, all we need to do is build and run:</p>

<pre><code>oj@nix ~/blog/csd $ make
==&gt; mochiweb (get-deps)
==&gt; webmachine (get-deps)
==&gt; protobuffs (get-deps)
==&gt; riakc (get-deps)
==&gt; erlydtl (get-deps)
==&gt; csd (get-deps)
==&gt; mochiweb (compile)
==&gt; webmachine (compile)
==&gt; protobuffs (compile)
==&gt; riakc (compile)
==&gt; erlydtl (compile)
==&gt; csd (compile)
Compiled src/csd_resource.erl
Compiled templates/sample.dtl

oj@nix ~/blog/csd $ ./start.sh 
Erlang R14B02 (erts-5.8.3) [source] [64-bit] [smp:2:2] [rq:2] [async-threads:0] [hipe] [kernel-poll:false]

  ... snip ...

** Found 0 name clashes in code paths 

  ... snip ...

=PROGRESS REPORT==== 3-Apr-2011::22:54:50 ===
         application: csd
          started_at: nonode@nohost
</code></pre>

<p>Notice how ErlyDTL outputs some information to indicate that no template names have clashed with any other modules.</p>

<p>The application is now running, let's see what it looks like:</p>

<p><img src="http://buffered.io/uploads/2010/09/wm_erlydtl.png"/></p>

<h3>The End</h3>

<p>We now have a working environment in which to do our development. In the next post, I'll cover some of the basics required to get Webmachine talking to Riak via <a href="http://en.wikipedia.org/wiki/Protocol_Buffers" title="Protocol Buffers">Protocol Buffers</a>.</p>

<p>Feedback and criticism welcome!</p>

<p><strong>Note:</strong> The code for Part 1 (this post) can be found on <a href="https://github.com/OJ/csd/tree/Part1-20110403" title="Source Code for Part 1">Github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting up Trac, Mercurial and SSH on Windows]]></title>
    <link href="http://buffered.io/posts/setting-up-trac-mercurial-and-ssh-on-windows/"/>
    <updated>2009-05-16T17:43:00+10:00</updated>
    <id>http://buffered.io/posts/setting-up-trac-mercurial-and-ssh-on-windows</id>
    <content type="html"><![CDATA[<p><strong>WARNING - This blog post is long :)</strong>
<em>This post has been edited since it was published. Please see the end of the article for any notes/modifications</em></p>

<h3>Some Background Info</h3>


<p>I had the need to do this for work recently. It was nothing short of a right royal pain in the butt. It was such a pain, in fact, that I have decided to document what I had to do to get it working so that other poor unfortunates will feel less pain if they have to do this themselves.</p>

<p>Almost regardless of the company and the software I'm working on, I use <a href="http://www.selenic.com/mercurial/" title="Mercurial">Mercurial</a> for source code control. For the work I am doing at the moment, I was also using hg because the company I am involved with is relatively new and they hadn't yet sorted out a plan for version control or <acronym title="Application Lifecycle Management">ALM</acronym>. It was working quite well and I was pushing all my changes to my <a href="http://www.google.com.au/url?q=http://www.buffalotech.com/products/network-storage/terastation/terastation-iii/&ei=fioOSuLvGpu8swPl56zqAg&sa=X&oi=smap&resnum=1&ct=result&cd=2&usg=AFQjCNHXKMchy5tR-a-jMOFAbtxoWzfedA" title="Terastation">NAS box</a> to make sure I had other copies backed up, etc. I was <em>living the dream</em> :)</p>

<!--more-->


<p>The problem, though, was that this setup was working fine for me as a sole developer, but wasn't solving the problem for the other developers. There are two other people involved in development, and until now most of the work they have been doing has been in a different area to the bits I have been doing. Sharing and merging code was done by email patches. Those developers weren't using version control as far as I'm aware.</p>

<p>So given the volume of changes and the number of merges that were happening, it was well past time to get something proper in place that we could all use. The original plan was to go with an installation of <a href="http://en.wikipedia.org/wiki/Team_Foundation_Server" title="Team Foundation Server">TFS</a> to handle all of these issues for us. Personally, I am not a huge fan of TFS. The experience I've had with it so far hasn't been great. It also has a shocking effect on the speed of my IDE (I seem to have no choice but to use integration with the IDE, which again isn't great).</p>

<p>To cut this part of the story short, we had issues getting TFS to work. We tried installing a few times, tweaking here and there, but we never managed to get it to work. I know I could have called a TFS guru, such as <a href="http://stevennagy.spaces.live.com/" title="Steven Nagy">Snagy</a> to pick his brains and perhaps ask him nicely to help out, but he's a busy man and there's no guarantee that we wouldn't have to start again from scratch. I wasn't really up for that, and neither was my boss.</p>

<p>So I told him that I thought it might just be easier to get a server running with Mercurial for the version control, and <a href="http://trac.edgewall.org/" title="The Trac Project">Trac</a> for issue tracking and project "stuff". Not only would it mean that I could just push my local repository to the server and have the entire history transferred (TFS would have been latest version only) but it would save us a substantial cost in license fees, remove the need for VS integration, and everyone could be up and running with a <acronym title="Distributed Version Control System">DVCS</acronym> in no time.</p>

<p>My boss, being the legend he is, gave me the thumbs up! Awesome.</p>

<p>So the rest of this post is dedicated to what was involved in getting things set up. If you're not interested, then go read <a href="http://buffered.io/?random" title="Random post">something else</a> :)</p>

<h3>The need for a bit of security</h3>


<p>Needless to say, the <acronym title="Intellectual Property">IP</acronym> that we're building is something that my employer is rather protective of! And rightly so. Hence it won't come as a surprise to hear that security of this information is very important. I decided that having Mercurial run over plan old HTTP would be a bad idea. I was keen to have it tunnel via SSH, and have all clients authenticate using their own private keys. Hence, getting an SSH server was going to be part of the setup.</p>

<h3>Web server</h3>


<p>IIS is already installed on the server, and running another web server seemed like overkill. I didn't want to expose <a href="http://trac.edgewall.org/wiki/TracStandalone" title="Trac Standalone">tracd</a> directly over the web as it doesn't support SSL, so I wanted to get it running under IIS instead. This added another little bit of complexity to the install.</p>

<h3>What to download</h3>


<p>There were quite a few bits that I needed to download to get this working, they're listed below. At first it might not be obvious as to why these things are needed, just trust me :)</p>

<p>First, server-side components:</p>

<ol>
<li>Python 2.5.4 - Get this version, not anything earlier or later if you want this guide to work! (<a href="http://www.python.org/ftp/python/2.5.4/python-2.5.4.msi" title="Download Python">download</a>)</li>
<li>Setuptools (<a href="http://pypi.python.org/packages/2.5/s/setuptools/setuptools-0.6c9.win32-py2.5.exe#md5=602d06054ec1165e995ae54ac30884d7" title="Download setuptools">download</a>)</li>
<li>ClearSilver (<a href="http://www.clearsilver.net/downloads/win32/clearsilver-0.10.4-py2.5-win32.egg" title="Download ClearSilver">download</a>)</li>
<li>PySqlite (<a href="http://initd.org/pub/software/pysqlite/releases/2.4/2.4.1/pysqlite-2.4.1.win32-py2.5.exe" title="Download PySqlite">download</a>)</li>
<li>flup (<a href="http://www.saddi.com/software/flup/dist/flup-0.5-py2.5.egg" title="Download flup">download</a>)</li>
<li>Trac installer v0.11.4 (<a href="http://ftp.edgewall.com/pub/trac/Trac-0.11.4.win32.exe" title="Download Trac">download</a>)</li>
<li>Mercurial (<a href="http://mercurial.berkwood.com/binaries/Mercurial-1.2.1.exe" title="Download Mercurial">download</a>)</li>
<li>Subversion (<a href="http://www.collab.net/servlets/OCNDirector?id=CSVN1.6.2WINC" title="Download Subversion">download</a>)</li>
<li>Apache Tomcat AJP Isapi filter (<a href="http://www.apache.org/dist/tomcat/tomcat-connectors/jk/binaries/win32/jk-1.2.28/isapi_redirect-1.2.28.dll" title="Download isapi filter">download</a>)</li>
<li>Junction (<a href="http://technet.microsoft.com/en-us/sysinternals/bb896768.aspx" title="Download Junction">download</a>)</li>
<li>CopSSH (<a href="http://sourceforge.net/project/downloading.php?group_id=69227&filename=Copssh_2.1.0_Installer.zip&a=22655277" title="Download CopSSH">download</a>)</li>
<li>Windows Server 2003 Resource Kit (<a href="http://www.microsoft.com/downloads/details.aspx?FamilyID=9D467A69-57FF-4AE7-96EE-B18C4790CFFD&displaylang=en" title="Download resource kit">download</a>)</li>
</ol>


<p>Next, client-side:</p>

<ol>
<li>Mercurial (<a href="http://mercurial.berkwood.com/binaries/Mercurial-1.2.1.exe" title="Download Mercurial">download</a>)</li>
<li>PuTTy (<a href="http://the.earth.li/~sgtatham/putty/latest/x86/putty-0.60-installer.exe" title="Download PuTTy">download</a>)</li>
</ol>


<p>Quite a lot isn't it :)</p>

<h3>Server: Setting up the Python Environment</h3>


<ol>
  <li>Execute the Python installer. This by default installs Python to C:\python25.</li>
  <li>Modify the environment variables so that the Python install folder is included in the PATH.</li>
  <li>Execute the Setuptools installer.</li>
  <li>Execute the PySqlite installer.</li>
  <li>Install the downloaded python eggs:</li>
  <ol>
    <li>Open a command prompt</li>
    <li>Navigate to the folder where your .egg files are downloaded to.</li>
    <li>in the command line, type: <strong>easy_install [filename.egg]</strong>.</li>
    <li>Do this for ClearSilver and flup.</li>
  </ol>
</ol>


<p>At this point, you should have a working installation of Python 2.5.4 with the required eggs and plug-ins.</p>

<h3>Server: Setting up Mercurial</h3>


<ol>
  <li>Execute the Mercurial installer. By default this installs to %programfiles%\Mercurial. Feel free to change this if you like.</li>
  <li>During the installation, the installer will ask if you want to include the install folder in the system PATH. Make sure you click "yes", otherwise you'll have to do it manually afterwards.</li>
</ol>


<p>The Mercurial client is now installed. Later on when installing Trac and getting it to work with Mercurial, I had issues because the Mercurial doesn't install the Mercurial bindings for Python. No only that, trying to use <em>easy_install</em> to install it didn't work because I didn't have Visual Studio 2003 installed on the server (nor did I want to install it!). It turns out that you can get round the issue by doing this:</p>

<ol>
  <li>Go to the Mercurial installation folder using Windows Explorer.</li>
  <li>Locate the file <strong>Library.zip</strong>.</li>
  <li>Open this file with an application like <a href="http://rarsoft.com/" title="WinRAR">WinRAR</a> or <a href="http://www.7-zip.org/" title="7-zip">7-zip</a> (using the built-in Windows zip functionality doesn't work!).</li>
  <li>Inside that archive there is a folder called <em>mercurial</em>. Extract this folder to a disk.</li>
  <li>Open another Windows Explorer and navigate to your Python installation directory (eg. C:\Python25). Open up the <em>Lib</em> sub0folder (that's <em>lib</em>, not <em>lib<strong>s</strong></em>).</li>
  <li>Copy or move the extracted <em>mercurial</em> folder to the <em>lib</em> folder.</li>
</ol>


<p>Done! You've just "installed" the Mercurial bindings for Python. Later down the track, Trac will not crap out when you try and use it!</p>

<h3>Server: Setting up TracMercurial</h3>


<p>Given that we're using Mercurial for the back-end version control, we're going to need to set up and install the Mercurial plugin for Trac. This is where the need for <acronym title="Subversion">SVN</acronym> comes in. Yes, it's ironic that you need SVN to get something working with Mercurial! Just do it, ok!?</p>

<ol>
  <li>Execute the Subversion installer and let it go through with the default installation.</li>
  <li>Make sure that the path to the SVN binaries is included in the system PATH environment variable.</li>
  <li>Open up a command prompt and change directory to a temporary location.</li>
  <li>run the command: <strong>svn co http://svn.edgewall.com/repos/trac/sandbox/mercurial-plugin-0.11</strong> - this gets the right version of the plug-in from source.</li>
  <li>run the command: <strong>cd mercurial-plugin-0.11</strong></li>
  <li>run the command: <strong>python setup.py bdist_egg</strong> - this creates an installable python egg.</li>
  <li>run the command: <strong>python setup.py install</strong> - this installs the plug-in to the global location.</li>
</ol>


<p>The TracMercurial plug-in is now installed. We'll need to configure it slightly when we've installed Trac. That information is listed in the next section.</p>

<p>Next we need to create a repository which will be used to house our source code. We'll pretend we're creating a project called "Slartibartfast".</p>

<ol>
  <li>Create a new folder on the file system somewhere meaningful, such as <strong>C:\Repos</strong>.</li>
  <li>Open a command window, and change directory to that folder.</li>
  <li>create a folder for your project: <strong>mkdir Slartibartfast</strong>.</li>
  <li>Change to that folder: <strong>cd Slartibartfast</strong>.</li>
  <li>Initialise the repository: <strong>hg init .</strong> (note the '.' at the end).</li>
</ol>


<p>Your repository is now ready for code!</p>

<h3>Server: Setting up Trac</h3>


<p>We'll start with the basic install:</p>

<ol>
  <li>Execute the Trac installer.</li>
  <li>Default installation settings are fine.</li>
  <li>It will be installed to the <em>scripts</em> folder under the Python installation folder.</li>
</ol>


<p>At this point it's a good idea to also add the <em>scripts</em> folder to the system PATH. This gives us the ability to run Python scripts from anywhere.</p>

<p>Next up, let's create a Trac project for our Slartitbartfast project.</p>

<ol>
  <li>Create a folder to house your Trac projects, say <strong>C:\Trac</strong></li>
  <li>Open a command window and change directory to your Trac projects folder.</li>
  <li>Create a folder for your new project: <strong>mkdir Slartibartfast</strong></li>
  <li>Change to that folder: <strong>cd Slartibartfast</strong>.</li>
  <li>Start the Trac administration application to start an interactive setup session: <strong>trac-admin . initenv</strong></li>
</ol>


<p>  You need to fill out some values as it asks for them. Here's a sample interactive session that I did as a test run:</p>

<pre><code>C:\Trac\Slartibartfast&gt;trac-admin . initenv
Creating a new Trac environment at C:\Trac\Slartibartfast

Trac will first ask a few questions about your environment
in order to initialize and prepare the project database.

 Please enter the name of your project.
 This name will be used in page titles and descriptions.

Project Name [My Project]&gt; Slartibartfast

 Please specify the connection string for the database to use.
 By default, a local SQLite database is created in the environment
 directory. It is also possible to use an already existing
 PostgreSQL database (check the Trac documentation for the exact
 connection string syntax).

Database connection string [sqlite:db/trac.db]&gt;

 Please specify the type of version control system,
 By default, it will be svn.

 If you don't want to use Trac with version control integration,
 choose the default here and don't specify a repository directory.
 in the next question.

Repository type [svn]&gt; hg

 Please specify the absolute path to the version control
 repository, or leave it blank to use Trac without a repository.
 You can also set the repository location later.

Path to repository [/path/to/repos]&gt; C:\Repos\Slartibartfast

Creating and Initializing Project
 Installing default wiki pages

**
** craploads of import statements go here **
**

---------------------------------------------------------------------
Warning: couldn't index the repository.

This can happen for a variety of reasons: wrong repository type,
no appropriate third party library for this repository type,
no actual repository at the specified repository path...

You can nevertheless start using your Trac environment, but
you'll need to check again your trac.ini file and the [trac]
repository_type and repository_path settings in order to enable
the Trac repository browser.


---------------------------------------------------------------------
Project environment for 'Slartibartfast' created.

You may now configure the environment by editing the file:

  C:\TracProjects\Slartibartfast\conf\trac.ini

If you'd like to take this new project environment for a test drive,
try running the Trac standalone web server `tracd`:

  tracd --port 8000 C:\TracProjects\Slartibartfast

Then point your browser to http://localhost:8000/Slartibartfast.
There you can also browse the documentation for your installed
version of Trac, including information on further setup (such as
deploying Trac to a real web server).

The latest documentation can also always be found on the project
website:

  http://trac.edgewall.org/

Congratulations!
</code></pre>

<p>You may notice the message: <em>Warning: couldn't index the repository.</em>
Don't be too concerned at this point, as we need to do a bit more to make sure that the Mercurial stuff is working.</p>

<p>Now we should test to make sure we're able to start the Trac daemon and see if the project site has been set up. We do that by executing the statement they suggested in the output:
<strong>tracd --port 8000 C:\TracProjects\Slartibartfast</strong>
Then we browse to <a href="http://localhost:8000">http://localhost:8000</a> to see if it works! If all goes well you should see a project listing. Click on Slartibartfast and you should see something like this:</p>

<p><a href="http://buffered.io/uploads/2009/05/trac_1.png"><img src="http://buffered.io/uploads/2009/05/trac_1.png" alt="Trac Project Page - with error." title="Trac Project Page - with error." width="640" height="347" /></a></p>

<p>Trac is telling us why it's not able to index the repository, it's because we haven't enabled the plug-in yet. Let's do that now.</p>

<p>Open up the <em>Trac.ini</em> file which is located inside the <em>conf</em> folder under the main project folder (eg <em>C:\Trac\Slartibartfast\conf\Trac.ini</em>).</p>

<p>First, add the following section to the ini file:</p>

<pre><code>[hg]
# -- Show revision number in addition to the changeset hash
show_rev = yes

# -- Changeset hash format
node_format = short
# hex:   Show the full SHA1 hash 
# short: Show a shortened hash for the changesets
</code></pre>

<p>Then enable the Mercurial plugin by adding the following:</p>

<pre><code>[components]
tracext.hg.backend.csetpropertyrenderer = enabled
tracext.hg.backend.hgdefaultpropertyrenderer = enabled
tracext.hg.backend.hgextpropertyrenderer = enabled
tracext.hg.backend.mercurialconnector = enabled
</code></pre>

<p><em>Note:</em>Make sure that the <kbd>[hg]</kbd> and <kbd>[components]</kbd> sections don't already exist. If they do, then add the respective lines to the existing sections rather than creating new ones. For a fresh install, these sections shouldn't already exist, so you should be safe to add them.</p>

<p>Kill the tracd instance by press CTRL+C, then restart it (press the up arrow, then enter). Refresh your browser window and this time you should see this:</p>

<p><a href="http://buffered.io/uploads/2009/05/trac_2.png"><img src="http://buffered.io/uploads/2009/05/trac_2.png" alt="Trac Project Page - without error." title="Trac Project Page - without error." width="640" height="372" /></a></p>

<p>Excellent, our project is up and our repository is able to be read. We're done!</p>

<h3>Server: Setting up the Trac Daemon as a Service</h3>


<p>The tracd needs to run as a service. This is to make sure it's always running in a way that IIS can reference it. Given that we're later going to be hooking this up to IIS, we need to make sure it uses the <a href="http://tomcat.apache.org/connectors-doc/ajp/ajpv13a.html" title="AJP Reference">AJP</a> protocol. Here are the required steps:</p>

<ol>
  <li>Install the Windows 2003 resource kit.</li>
  <li>Open a command window, and navigate to where the resource kit binaries are installed. On my system that was at <em>C:\Program Files\Windows Resource Kits\Tools</em></li>
  <li>Create a service called "Trac" by executing the following command: <strong>InstSrv Trac "C:\Program Files\Windows Resource Kits\Tools\SrvAny.exe"</strong> - (note the use of the full path is necessary for this to work).</li>
  <li>Next we need to hack the registry a little to pass the right parameters and have the tracd started.
<ol>
  <li>Open regedit.</li>
  <li>Go to <strong>HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Trac</strong></li>
  <li>Create a new sub-key called <strong>Parameters</strong></li>
  <li>Inside this new sub-key, create two new values:
  </li>
<ol>
  <li><strong>Application</strong> of type String. Set the value to the full path of <em>python.exe</em>. Eg <strong>C:\python25\python.exe</strong></li>
  <li><strong>AppParameters</strong> of type String. Set the value to <strong>C:\python25\Scripts\tracd-script.py --port 8009 C:\Trac\Slartibartfast</strong> - (note the change of port number, it's there for a reason we'll cover shortly).</li>
</ol>
</ol>
<li>Open the services tool from the Control Panel, find the <strong>Trac</strong> service and start it up! Navigate to <a href="http://localhost:8009">http://localhost:8009</a> to make sure it still works.</li>
</ol>


<p>If you get the same site as you did before, then all is going well. If not, then you revisit the steps to make sure you haven't missed anything out and that your pointing to the right location of the trac project.</p>

<p>Now that we know this works, we need to change the <strong>AppParameters</strong> value in the registry so that Trac fires up using the appropriate protocol. Edit the value, and append the following to the end of the existing value: <strong>--protocol=ajp</strong> - this forces the use of AJP. AJP's default port is 8009, which is why we chose this value in the first place.</p>

<p>Restart the Trac service. If you attempt to browse to the site now you won't get anything meaningful, but it's ready to be talked to from IIS, which brings us to the next step.</p>

<h3>Server: Setting up Apache Tomcat ISS ISAPI Filter in IIS 6.</h3>


<p>To get IIS to talk to Trac, we need to add an ISAPI filter that's able to translate to the AJP protocol. This is what we'll be doing in this section.</p>

<p>First, we prepare the filter for use:</p>

<ol>
  <li>Create a folder to store the ISAPI file. I called mine <strong>C:\AJPConnector</strong>.</li>
  <li>Copy the Apache Tomcat AJP ISAPI filter (isapi_redirect-1.2.28.dll) into the folder you created in the previous step, and rename it to <strong>isapi_redirect.dll</strong>.</li>
  <li>Create a new text file in the same folder called <strong>isapi_redirect.properties</strong>. Open the file with a text editor and add the following content:

    # Configuration file for the ISAPI Redirector

    # The path to the ISAPI Redirector Extension, relative to the website
    # This must be in a virtual directory with execute privileges
    extension_uri=/AJPConnector/isapi_redirect.dll

    # Full path to the log file for the ISAPI Redirector
    log_file=C:\AJPConnector\isapi_redirect.log

    # Log level (debug, info, warn, error or trace)
    log_level=info

    # Full path to the workers.properties file
    worker_file=C:\AJPConnector\workers.properties

    # Full path to the uriworkermap.properties file
    worker_mount_file=C:\AJPConnector\uriworkermap.properties

</li>
<li>Create another new text file in the same folder called <strong>workers.properties</strong>. Open it, and add this content:

    # Define 1 real worker
    worker.list=trac
    # Set properties for trac (ajp13)
    worker.trac.type=ajp13
    worker.trac.host=localhost
    worker.trac.port=8009
    worker.trac.socket_keepalive=0

</li>
<li>Create <em>another</em> new text file in the same folder called <strong>uriworkermap.properties</strong>. Open it and add the following:

    /Slartibartfast*=trac

You'll notice that the project name is what I've used here. Down the track if you want more projects, you need to add more lines to this file.</li>
</ol>


<p>Next we need to install the ISAPI filter in IIS so that it can get used. Feel free to do this on the default website if you like. I created another website in IIS running on a different port, but you don't have to. Any time I refer to <em>the website</em> I'm referring to the site you have decided to install the filter on.</p>

<p>We need to enable the filter as a web service extension first. To do that, we follow these steps:</p>

<ol>
  <li>Open up the IIS manager.</li>
  <li>Right-click on "Web Service Extensions" and select "Add a new web service extension".</li>
  <li>In the resulting dialog box, click the "Add..." button and specify the file <strong>C:\AJPConnector\isapi_redirect.dll</strong>. Click "OK".</li>
  <li>Specify the name "AJPConnector" and click "OK".</li>
</ol>


<p>Next we need to add the filter to the website.</p>

<ol>
  <li>In IIS manager right-click on the website and view the properties.</li>
  <li>Select the "ISAPI Filters" tab.</li>
  <li>Click "Add".</li>
  <li>Give the filter the name "AJPConnector".</li>
  <li>Specify the full path to the dll: <strong>C:\AJPConnector\isapi_redirect.dll</strong>.</li>
  <li>Click "OK".</li>
  <li>Click on the "Home Directory" tab.</li>
  <li>Make sure the "Execute Permissions" is set to "Scripts and Executables".</li>
  <li>Make a note of the application pool that the web site is running under! You'll need this in a minute.
  <li>Click "OK" to close the dialog.</li>
</ol>


<p>For the ISAPI filter to function, we need to make sure that IIS has access to the folder which the binary is stored in. To do this we need to know which account the website is running under.</p>

<ol>
  <li>In IIS manager open the list of Application Pools.</li>
  <li>Choose the application pool that your website is running under. Rick-click and select properties.</li>
  <li>Click on the "Identity" tab. This will show you the name of the account that your site is running under.</li>
  <li>Browse to <strong>C:\AJPConnector</strong>, right-click and select "Properties", then choose the "Security" tab.</li>
  <li>Click "Add" and type in the name of the account that you found listed in the "Identity" tab in the application pool properties. You can also use the search feature if you want to.</li>
  <li>When the user has been recognised, click "OK".</li>
  <li>Select the user in the list at the top of the dialog, and make sure that they have "Full Control" selected in the list at the bottom.</li>
  <li>Click on the "Advanced" button.</li>
  <li>Select the check box that says "Replace permission entries on all child objects with entries shown here that apply to child objects". Then press "OK". This will apply the permissions to all child objects in the folder.</li>
  <li>Click "OK" to close the dialog box.</li>
</ol>


<p>You may have noticed that in the <strong>isapi_redirect.properties</strong> file we specified an <em>extension_uri</em> property. This property specifies the path to use to get to the <strong>isapi_redirect.dll</strong> file via IIS. Notice how it's also a relative path from the root folder. Hence we need to create a virtual directory which maps to the same path.</p>

<ol>
  <li>In IIS manager right-click on the website and select "New" then "Virtual Directory...".</li>
  <li>Fill out the wizard making sure you use the name "AJPConnector" for the directory name, and point it at <strong>C:\AJPConnector</strong>. The site also needs to be able to execute programs, not just scripts.</li>
</ol>


<p>Restart IIS. The quickest way to do this is to type <strong>iisreset</strong> into a command prompt or into the "Run" dialog.</p>

<p>It's best at this point to make sure that the ISAPI filter has been loaded. To do this, simply open up the IIS manager again and browser to the sites "ISAPI Filters" tab. If all is working, then you should see something like this:</p>

<p><img src="http://buffered.io/uploads/2009/05/iis_isapi.png" alt="Working IIS ISAPI Filter" title="Working IIS ISAPI Filter" /></p>

<p>Notice the green arrow. If it's red and pointing downwards, then something is wrong. You need to make sure you have followed the above steps correctly. Double-check the security of the folders and make sure executables are enabled on your site.</p>

<p>If all is well then we can test it! Make sure your Trac service is running, then open a browser and navigate to your local IIS website. You should see the Trac Project website for Slartibartfast appear in your browswer. Yay!</p>

<p>It's now very easy to change IIS to function over HTTPS instead of HTTP. That's beyond the scope of this article as there are hundreds of blog entries and how-tos out there already.</p>

<h3>Server: Setting up CopSSH</h3>


<p>This is the final step! We need to give people access to the Mercurial repositories over SSH. For that we need to set up a functional SSH server and give them access to the repository path. On the surface these both seem like easy jobs. Unfortunately setting up SSH servers on Windows isn't pleasant and pointing everyone's home folders at a certain repository is not something that comes out of the box in Windows.</p>

<p>What we need to do is use the CopSSH installer to help with the "easy" installation of SSH. Then we need to use a program like Junction to provdie symbolic-link style functionality to the user's folders.</p>

<p>First up, run the CopSSH installer program. Follow all of the prompts and feel free to use the default installation. That was easy wasn't it! It should install an SSH server service for you. I found it quite hard to locate in the Service Management dialog, but eventually found it under the name "Openssh SSHD".</p>

<p>CopSSH comes with utilities which help you manage users. Those utilities are all in the programs menu and are easy to use. For me, those utilities weren't enough, because I knew that I was going to have to make sure that the folder links were set up on creation of user accounts and removed when those accounts were deleted.</p>

<p>It's a good move at this point to download Junction, if you haven't already, and put it in an easy to reach location. Feel free to add it to it's own folder or to system32. It's up to you. I'll assume you've put it in <strong>C:\tools</strong>.</p>

<p>I then created two scripts. The first lets you add users. It looks like this:</p>

<pre><code>REM AddUser.bat
"C:\Program Files\ICW\Bin\copsshadm.exe" --command activateuser --user %1 --shell /bin/bash --passphrase %2
C:\tools\junction.exe "C:\Program Files\ICW\home\%1\Slartibartfast" C:\Repos\Slartibartfast
</code></pre>

<p>The first line of this file executes the user activation feature of CopSSH. It creates a new user from a local security login (so the users need to be accessible in Active Directory or in the local security set up). It creates a home folder for them and also creates a public/private key pair which it stores in their home folder as well. The keys have the passphrase that is passed to the script on the command line and will be used by the users to SSH in and commit changes to the repository. The public key for the user is added to the server as a recognised key which can be used to log in so we don't have to specify it manually.</p>

<p>The second line of the script executes the Junction program and links a folder called "Slartibartfast", located in the user's home folder, to the folder where the repository is stored.</p>

<p>To execute the script simply run: <strong>adduser.bat [username] [passphrase]</strong>.
Please note that during my testing, I wasn't able to use a passphrase that had spaces. This isn't due to a bug in the script. Even if you replace <em>--passphrase %2</em> with <em>--passphrase "%2"</em> it still doesn't work. It also doesn't work in the user interface tools. This appears to be a bug in CopSSH. This isn't a big issue though, just make sure you don't have spaces in your passphrase and you should be fine.</p>

<p>The second script is for removing users, it looks like this:</p>

<pre><code>REM RemUser.bat
C:\tools\junction.exe -d "C:\Program Files\ICW\home\%1\Slartibartfast"
"C:\Program Files\ICW\Bin\copsshadm.exe" --command deactivateuser --user %1
</code></pre>

<p>It's obvious what this does. First it removes the symbolic link in the user's folder, then it uses CopSSH's command line tools to remove them as a valid user from the SSH server.</p>

<p>Before you assume that everything at this point is ready to rock, you may need to make sure that your firewall is updated so that the SSH port, 22, is open.</p>

<p>Congratulations, you are now finished with the server set up. Now let's get the clients working. Before you go to the next step, make sure you add a user!</p>

<h3>Client: Setting up PuTTy</h3>


<p>The first bit is easy, make sure that you've installed the PuTTy program and added the install path to the system's PATH environment variable.</p>

<p>Take a copy of the user's private key. This is generated by CopSSH and should be sitting in the user's home folder on the server. The key is usually called <strong>[username].key</strong>. Put the key on your local machine. We need to convert this key into a format that PuTTy can use.</p>

<ol>
  <li>Fire up the puttygen.exe utility. It comes with the PuTTy program. Simply type "puttygen" into the Run dialog box and it should come up if you've added the install folder to the system PATH.</li>
  <li>Click "Load" and select the private key file you copied from the server.</li>
  <li>Enter the user's passphrase into the dialog that appears and press "OK".</li>
  <li>You should get a message stating ...
  

    Successfully imported foreign key
    (OpenSSH SSH-2 private key).
    To use this key with PuTTY, you need to
    use the "Save private key" command to
    save it in PuTTY's own format.

Just press "OK".</li>
<li>Click "Save private key" and save the key in a safe location on your machine.</li>
</ol>


<p>Your key can now be used by PuTTy.</p>

<p>When using SSH, you are going to have to have to specify the passphrase each time you connect to the server. This can get annoying quickly especially if you do a lot of commits. Instead of typing this in manually every time, I prefer to use the pageant.exe utility that also comes with PuTTy. This takes charge of handling the key passphrase for you.</p>

<ol>
  <li>From the command line or Run dialog, type <strong>pageant</strong>. You see a rather bland dialog appear.</li>
  <li>Click "Add key". Browse to the folder where you have stored your new .ppk (the PuTTy version of your key) and select the key.</li>
  <li>When prompted, specify the passphrase and click "OK".</li>
  <li>You should now see an item appear in the list box. This is your private key. Pageant is now in memory and handling that key.</li>
  <li>Close pageant, but you should still see it running in the system tray.</li>
</ol>


<p>I also find it helpful to add <em>pageant.exe</em> to my start-up folder so that it is always running when I log in to my machine. You may choose to do the same.</p>

<h3>Client: Setting up Mercurial</h3>


<p>If you haven't already, make sure you install Mercurial on the client machine using the default settings. Make sure you tell the installer to add the Mercurial path to the system PATH.</p>

<p>The last step of configuration for the client is to tell Mercurial to use the PuTTy tools when using SSH. Mercurial can be configured by a user-specific configuration file called <em>.hgrc</em>. On Windows it can also be called <em>Mercurial.ini</em>. The file is located in your home folder. If you don't know what your home folder is, simply open a command prompt and type <strong>echo %USERPROFILE%</strong> - this will tell you the path.</p>

<p>If you haven't set up your configuration yet, then chances are the configuration file doesn't exist. So you'll have to create it. Create a file call either <em>.hgrc</em> or <em>Mercurial.ini</em> in your home folder manually, and open it in a text editor. Here is what part of mine looks like:</p>

<pre><code>[ui]
username = OJ Reeves &lt;put your email here in the angle brackets&gt;
editor = vim
ssh = plink -ssh -i "C:/path/to/key/id_rsa.ppk" -C -agent
</code></pre>

<p>The last line is the key and this is what you need to make sure it set properly. We are telling Mercurial to use the <em>plink</em> program. This also comes with PuTTy and is a command-line version of what the PuTTY program itself does behind the scenes. We also add a few parameters:</p>

<ul>
  <li><em>-ssh</em> : Indicates that we're using the SSH protocol.</li>
  <li><em>-i "file.ppk"</em> : Specifies the location of the private key file we want to use to log in to the remote server. Change this to point to your local putty-compatible ppk private key. Make sure you user forward-slashes for the path separators as well!</li>
  <li><em>-C</em> : This switch enables compression.</li>
  <li><em>-agent</em> : This tells <em>plink</em> to talk to the <em>pageant</em> utility to get the passphrase for the key instead of asking you for it interactively.</li>
</ul>


<p>The client is now ready to rock!</p>

<h3>Finale</h3>


<p>It took a while, but we got there in the end. Let's give our new-found Mercurial SSH server a spin. This should be easy as opening a command prompt at the appropriate location on disk and typing:</p>

<p><strong>hg clone ssh://url.to.your.server.com/Slartibartfast</strong></p>

<p>If everything has been configured properly, you should see Mercurial create a local folder called <strong>Slartibartfast</strong> which contains the repository. Of course, there won't be much in it because you've only created it recently! But you should be able to start using Mercurial to commit changes and push the committed changesets to the server just by running <strong>hg push</strong>.</p>

<h3>The End</h3>


<p>It took a while to figure most of this stuff out despite the documentation that exists on the web. It also took a bit of time to write this up! I hope that someone out there finds it useful.</p>

<p>The funny thing about all this, is that despite the pain, I found it easier to set up than TFS!</p>

<p>Feedback is always wanted and welcome. I hope it helps :)</p>

<hr />


<p><strong>Edit 11/06/2009 :</strong> I've been using this set up for a little while and I'm now aware of an issue that I'm not currently able to get around (moreso because I can't be bothered trying to!). The issue is that you have <em>any</em> files added to the Trac system which have file names containing characters which change when URL encoded (such as spaces, which become %20 for example) the you won't be able to view them through the web interface. That goes for files attached to wiki pages, and sources files viewed in the source browser. If you have a file called "My Doc.doc" behind the scenes this setup results calls being made with the file name "My%20Doc.doc", which doesn't exist! Just be warned. If you're going to use this setup, don't add files to your source or to the wiki that contain spaces or odd characters. :)</p>

<hr />


<p><strong>Edit 26/06/2009 :</strong>
Have a look at <a href="http://buffered.io/posts/setting-up-trac-mercurial-and-ssh-on-windows/#comment-20599851" title="Jeremy's comment">Jeremy's comment</a> for a resolution to this escaping problem. Thanks for that mate!</p>
]]></content>
  </entry>
  
</feed>
